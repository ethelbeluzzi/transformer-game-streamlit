import streamlit as st
import time
import datetime
import os

# --- Fun√ß√µes Auxiliares para Logs ---
LOG_FILE = "game_feedback.log"

def log_feedback(feedback_text):
    """
    Registra o feedback do usu√°rio em um arquivo de log.
    No ambiente do Streamlit Cloud, este arquivo √© tempor√°rio para a sess√£o.
    Para persist√™ncia real, seria necess√°rio um servi√ßo de armazenamento externo.
    """
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with open(LOG_FILE, "a") as f:
        f.write(f"[{timestamp}] {feedback_text}\n")
    st.success("Obrigado pelo seu feedback! Ele foi registrado.")

# --- Conte√∫do do Jogo ---

def main_menu():
    # T√≠tulo modificado para confirmar a atualiza√ß√£o da vers√£o
    st.title("üöÄ A Jornada do Transformer: Aten√ß√£o Desvendada! (V3) üöÄ")
    st.markdown("Bem-vindo, **engenheiro de IA**! Sua miss√£o √© construir o modelo de tradu√ß√£o de linguagem mais eficiente e poderoso do mundo. Guie seu **Transformer** atrav√©s das fases de design, treinamento e otimiza√ß√£o.")
    # Mantendo apenas a imagem principal da arquitetura
    st.image("https://upload.wikimedia.org/wikipedia/commons/thumb/1/1d/Transformer_architecture.svg/800px-Transformer_architecture.svg.png", use_container_width=True, caption="Arquitetura do Transformer: Onde a Aten√ß√£o √© Tudo!")
    st.write("Prepare-se para desvendar os segredos da aten√ß√£o!")
    # Mudando a key do bot√£o inicial para evitar qualquer confus√£o te√≥rica
    if st.button("Iniciar Miss√£o ‚û°Ô∏è", key="main_menu_start_button"):
        print("Bot√£o 'Iniciar Miss√£o' clicado!") # Log para depura√ß√£o
        st.session_state.game_state = "phase1"
        st.rerun()

    report_bug_section()


def phase1_architecture():
    st.header("Fase 1: A Arquitetura Fundacional (Encoder-Decoder) üèóÔ∏è")
    st.write("Antes do Transformer, a maioria dos modelos de sequ√™ncia usava redes recorrentes (RNNs) ou convolucionais (CNNs) para processar informa√ß√µes. No entanto, elas tinham limita√ß√µes, especialmente na **paraleliza√ß√£o do treinamento** para sequ√™ncias longas.")
    st.write("Qual tipo de arquitetura voc√™ escolher√° para o cora√ß√£o do seu novo modelo?")

    with st.expander("ü§î Explicar de forma simples: O que √© um Encoder-Decoder?"):
        st.write("Imagine que voc√™ quer traduzir uma frase. O **Encoder** √© como um *leitor* que l√™ a frase original inteira e a 'entende', transformando-a em um c√≥digo ou representa√ß√£o. O **Decoder** √© como um *escritor* que, com base no que o leitor entendeu (o c√≥digo), escreve a frase traduzida, palavra por palavra. Eles trabalham juntos para transformar uma sequ√™ncia de entrada (frase original) em uma sequ√™ncia de sa√≠da (frase traduzida).")

    col1, col2, col3 = st.columns(3)
    with col1:
        if st.button("Redes Recorrentes Complexas üîÑ", key="p1_btn_rnn"):
            st.error("‚ùå Resposta incorreta! Redes recorrentes s√£o inerentemente sequenciais, o que impede a paraleliza√ß√£o eficiente dentro dos exemplos de treinamento, tornando-as lentas para sequ√™ncias longas.")
            if 'p1_attempts' not in st.session_state: st.session_state.p1_attempts = 0
            st.session_state.p1_attempts += 1
    with col2:
        if st.button("Redes Convolucionais üñºÔ∏è", key="p1_btn_cnn"):
            st.warning("‚ö†Ô∏è Resposta aceit√°vel, mas n√£o a ideal! Modelos convolucionais podem processar em paralelo, mas a capacidade de relacionar posi√ß√µes distantes cresce linearmente ou logaritmicamente com a dist√¢ncia, dificultando o aprendizado de depend√™ncias de longo alcance de forma ideal.")
            if 'p1_attempts' not in st.session_state: st.session_state.p1_attempts = 0
            st.session_state.p1_attempts += 1
    with col3:
        if st.button("Aten√ß√£o Pura (Transformer) ‚ú®", key="p1_btn_attention"):
            st.success("‚úÖ Correto! O Transformer se baseia unicamente em mecanismos de aten√ß√£o, abandonando a recorr√™ncia e as convolu√ß√µes inteiramente. Isso permite muito mais paraleliza√ß√£o e um treinamento significativamente mais r√°pido. ****")
            st.write("O Transformer segue uma arquitetura Encoder-Decoder, usando pilhas de auto-aten√ß√£o. ****")
            # Mantendo apenas a imagem principal da arquitetura aqui tamb√©m
            st.image("https://upload.wikimedia.org/wikipedia/commons/thumb/1/1d/Transformer_architecture.svg/800px-Transformer_architecture.svg.png", use_container_width=True, caption="Diagrama Simplificado do Encoder-Decoder")
            st.write("Pr√≥xima etapa: mergulhar no cora√ß√£o da aten√ß√£o!")
            # Este bot√£o deve aparecer APENAS se a resposta correta foi escolhida
            if st.button("Avan√ßar para Fase 2 ‚û°Ô∏è", key="p1_advance_button"): # Adicionado key
                print("Bot√£o 'Avan√ßar para Fase 2' clicado!") # Log para depura√ß√£o
                st.session_state.game_state = "phase2"
                st.rerun()

    if st.session_state.get('p1_attempts', 0) >= 3 and st.session_state.game_state != "phase2":
        st.info("üí° Dica: Lembre-se que o Transformer 'dispensa' recorr√™ncia e convolu√ß√µes para focar em paralelismo.")
    report_bug_section()


def phase2_scaled_dot_product_attention():
    st.header("Fase 2: O Poder da Aten√ß√£o (Scaled Dot-Product Attention) üéØ")
    st.write("A fun√ß√£o de aten√ß√£o mapeia uma Query (Q) e um conjunto de pares Key-Value (K, V) para uma sa√≠da, que √© uma soma ponderada dos Values. ****")
    st.write("Nosso modelo usa a 'Scaled Dot-Product Attention'. Ela calcula os produtos escalares da Query com todas as Keys, divide cada um por $\\sqrt{d_k}$, e aplica uma fun√ß√£o softmax para obter os pesos.")

    st.latex(r"Attention(Q,K,V)=softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V \quad (1)")

    with st.expander("ü§î Explicar de forma simples: O que s√£o Q, K, V em Aten√ß√£o?"):
        st.write("  **Q (Query - Pergunta):** √â como a pergunta que voc√™ est√° fazendo para entender uma palavra. Para cada palavra que voc√™ quer processar, ela age como uma 'pergunta' sobre a relev√¢ncia de outras palavras.")
        st.write("  **K (Key - Chave):** S√£o as 'chaves' para as respostas. Cada palavra na sequ√™ncia tem uma 'chave' que a descreve ou a identifica.")
        st.write("  **V (Value - Valor):** S√£o os 'valores' ou as informa√ß√µes associadas a cada 'chave'. Se uma 'chave' √© relevante para a 'pergunta' (Q), seu 'valor' (V) ser√° mais considerado na resposta final.")
        st.write("A aten√ß√£o √© basicamente o processo de encontrar quais 'chaves' (palavras) s√£o mais relevantes para a 'pergunta' (palavra atual) e usar seus 'valores' para construir uma nova e melhor representa√ß√£o da palavra.")

    with st.expander("ü§î Explicar de forma simples: Por que a escala por $\\sqrt{d_k}$?"):
        st.write("Quando as dimens√µes ($d_k$) de Q e K s√£o grandes (os vetores s√£o longos), os produtos escalares (multiplica√ß√µes de Q e K) podem se tornar n√∫meros muito grandes. Isso pode 'saturar' a fun√ß√£o `softmax`, que √© usada para transformar esses produtos em pesos (probabilidades entre 0 e 1). Quando o `softmax` satura, seus gradientes (que s√£o usados para o modelo aprender) ficam extremamente pequenos, e o modelo pode aprender muito lentamente ou at√© parar de aprender. Dividir por $\\sqrt{d_k}$ 'diminui' esses produtos, mantendo-os em uma faixa mais est√°vel onde o `softmax` funciona melhor, evitando que os gradientes fiquem min√∫sculos. ****")

    st.write("Qual √© o principal motivo para dividirmos por $\\sqrt{d_k}$?")

    col1, col2 = st.columns(2)
    with col1:
        if st.button("Aumentar a magnitude dos produtos escalares", key="p2_btn_increase"):
            st.error("‚ùå Incorreto! Na verdade, √© o oposto. Para grandes valores de $d_k$, os produtos escalares tendem a crescer muito em magnitude naturalmente.")
            if 'p2_attempts' not in st.session_state: st.session_state.p1_attempts = 0
            st.session_state.p1_attempts += 1
    with col2:
        if st.button("Evitar gradientes muito pequenos no softmax ‚úÖ", key="p2_btn_softmax"):
            st.success("‚úÖ Correto! Para grandes valores de $d_k$, os produtos escalares crescem muito em magnitude, empurrando a fun√ß√£o softmax para regi√µes com gradientes extremamente pequenos. A escala por $\\frac{1}{\\sqrt{d_k}}$ contrai esse efeito, garantindo um treinamento mais est√°vel. ****")
            st.write("Excelente! A 'Scaled Dot-Product Attention' √© fundamental para a estabilidade e o desempenho do Transformer.")
            if st.button("Avan√ßar para Fase 3 ‚û°Ô∏è", key="p2_advance_button"): # Adicionado key
                print("Bot√£o 'Avan√ßar para Fase 3' clicado!") # Log para depura√ß√£o
                st.session_state.game_state = "phase3"
                st.rerun()

    if st.session_state.get('p2_attempts', 0) >= 2 and st.session_state.game_state != "phase3":
        st.info("üí° Dica: Pense no que acontece com os valores quando $d_k$ √© grande e qual fun√ß√£o √© usada para obter os pesos (softmax).")
    report_bug_section()


def phase3_multi_head_attention():
    st.header("Fase 3: Multi-Head Attention: M√∫ltiplas Perspectivas üí°")
    st.write("Em vez de uma √∫nica fun√ß√£o de aten√ß√£o, o Transformer usa 'Multi-Head Attention'. Isso permite que o modelo 'atenda conjuntamente a informa√ß√µes de diferentes subespa√ßos de representa√ß√£o em diferentes posi√ß√µes'. ****")
    st.write("Projetamos Q, K e V `h` vezes linearmente para dimens√µes menores ($d_k$, $d_v$), executamos a aten√ß√£o em paralelo, e concatenamos os resultados.")
    st.latex(r"""
    MultiHead(Q,K,V)=Concat(head_{1},...,head_{h})W^{O} \\
    \text{where } head_{i}=\text{Attention}(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V}) \quad
   """)

    with st.expander("ü§î Explicar de forma simples: Qual a vantagem de ter m√∫ltiplas cabe√ßas de aten√ß√£o?"):
        st.write("Imagine que voc√™ est√° tentando entender uma frase complexa. Uma √∫nica 'cabe√ßa de aten√ß√£o' pode focar em uma coisa espec√≠fica (ex: qual verbo est√° ligado a qual sujeito). Mas se voc√™ tiver v√°rias 'cabe√ßas', cada uma pode focar em algo diferente ao mesmo tempo (ex: uma no sujeito-verbo, outra no objeto-verbo, outra em sin√¥nimos ou rela√ß√µes mais abstratas).")
        st.write("A 'Multi-Head Attention' permite que o Transformer olhe para a mesma informa√ß√£o de v√°rias maneiras diferentes ao mesmo tempo, capturando rela√ß√µes mais ricas, diversas e complexas entre as palavras. E a melhor parte √© que, mesmo com v√°rias cabe√ßas, o custo computacional √© mantido eficiente. ****")

    st.write("Se $d_{model}=512$ e usamos $h=8$ cabe√ßas, quais seriam as dimens√µes de $d_k$ e $d_v$ para cada cabe√ßa, para que o custo computacional total seja similar ao de uma √∫nica cabe√ßa com dimensionalidade total?")

    d_val = st.slider("Escolha o valor para d_k e d_v (esperado d_model/h)", 32, 128, 64, step=32, key="p3_slider")

    if d_val == 64:
        st.success(f"‚úÖ Correto! Com $d_{{model}}=512$ e $h=8$, ent√£o $d_k = d_v = d_{{model}}/h = 512/8 = 64$. ****")
        st.write("Isso permite que o modelo aprenda diferentes representa√ß√µes e foque em diferentes aspectos da sequ√™ncia, sem aumentar significativamente o custo computacional.")
        # Mantendo apenas a imagem principal da arquitetura aqui tamb√©m
        st.image("https://upload.wikimedia.org/wikipedia/commons/thumb/1/1d/Transformer_architecture.svg/800px-Transformer_architecture.svg.png", use_container_width=True, caption="Multi-Head Attention")
        if st.button("Avan√ßar para Fase 4 ‚û°Ô∏è", key="p3_advance_button"): # Adicionado key
            print("Bot√£o 'Avan√ßar para Fase 4' clicado!") # Log para depura√ß√£o
            st.session_state.game_state = "phase4"
            st.rerun()
    else:
        st.warning(f"Tente novamente. Lembre-se que o custo computacional √© mantido similar ao de uma √∫nica cabe√ßa de aten√ß√£o com dimensionalidade total.")
        if 'p3_attempts' not in st.session_state: st.session_state.p3_attempts = 0
        st.session_state.p3_attempts += 1
        if st.session_state.get('p3_attempts', 0) >= 2:
            st.info("üí° Dica: Divida a dimens√£o total (`d_model`) pela quantidade de cabe√ßas (`h`).")
    report_bug_section()


def phase4_positional_encoding():
    st.header("Fase 4: A Import√¢ncia da Posi√ß√£o (Positional Encoding) üìç")
    st.write("Como o Transformer n√£o possui recorr√™ncia ou convolu√ß√£o, ele precisa de uma forma de saber a ordem das palavras na sequ√™ncia. ****")
    st.write("Para isso, adicionamos 'encodings posicionais' aos embeddings de entrada.")

    with st.expander("ü§î Explicar de forma simples: Por que o Positional Encoding √© necess√°rio?"):
        st.write("Modelos como RNNs processam as palavras uma de cada vez, ent√£o eles naturalmente 'sabem' a ordem em que as palavras aparecem. O Transformer, por outro lado, processa todas as palavras ao mesmo tempo e n√£o tem uma no√ß√£o 'intr√≠nseca' de ordem.")
        st.write("Sem alguma informa√ß√£o de posi√ß√£o, ele n√£o conseguiria diferenciar frases como 'O gato perseguiu o rato' de 'O rato perseguiu o gato', pois ambas teriam as mesmas palavras, apenas em ordens diferentes.")
        st.write("O Positional Encoding adiciona um 'carimbo de posi√ß√£o' √∫nico a cada palavra, que carrega informa√ß√µes sobre sua posi√ß√£o na sequ√™ncia e sua dist√¢ncia em rela√ß√£o a outras palavras. Isso permite que o modelo entenda a sintaxe e a sem√¢ntica da frase baseada na ordem das palavras. ****")

    st.write("Que tipo de fun√ß√£o o paper utilizou para gerar esses encodings posicionais?")

    col1, col2, col3 = st.columns(3)
    with col1:
        if st.button("Embeddings Posicionais Aprendidos", key="p4_btn_learned"):
            st.warning("‚ö†Ô∏è O paper experimentou isso, mas encontrou resultados quase id√™nticos ao m√©todo escolhido e optou por outro devido √† capacidade de extrapola√ß√£o para sequ√™ncias mais longas.")
            if 'p4_attempts' not in st.session_state: st.session_state.p4_attempts = 0
            st.session_state.p4_attempts += 1
    with col2:
        if st.button("Fun√ß√µes Seno e Cosseno de Diferentes Frequ√™ncias ‚úÖ", key="p4_btn_sin_cos"):
            st.success("‚úÖ Correto! O paper usou fun√ß√µes seno e cosseno de diferentes frequ√™ncias para gerar os encodings posicionais. ****")
            st.write("Cada dimens√£o do encoding posicional corresponde a uma sinusoide, e isso permite que o modelo aprenda facilmente a atender por posi√ß√µes relativas, pois os encodings formam uma progress√£o geom√©trica.")
            # **IMAGEM REMOVIDA AQUI, CONFORME SOLICITADO**
            if st.button("Avan√ßar para Fase 5 ‚û°Ô∏è", key="p4_advance_button"): # Adicionado key
                print("Bot√£o 'Avan√ßar para Fase 5' clicado!") # Log para depura√ß√£o
                st.session_state.game_state = "phase5"
                st.rerun()
    with col3:
        if st.button("Hash de Posi√ß√£o", key="p4_btn_hash"):
            st.error("‚ùå Incorreto. Essa n√£o foi a t√©cnica utilizada no paper original para o Positional Encoding.")
            if 'p4_attempts' not in st.session_state: st.session_state.p4_attempts = 0
            st.session_state.p4_attempts += 1

    if st.session_state.get('p4_attempts', 0) >= 2 and st.session_state.game_state != "phase5":
        st.info("üí° Dica: O m√©todo escolhido foi para permitir extrapola√ß√£o para sequ√™ncias mais longas e n√£o depender de treinamento.")
    report_bug_section()


def phase5_training_results():
    st.header("Fase 5: Treinamento e Otimiza√ß√£o (Resultados e Efici√™ncia) ‚ö°")
    st.write("O Transformer n√£o s√≥ √© inovador em sua arquitetura, mas tamb√©m em seu desempenho de treinamento e nos resultados finais. ****")
    st.write("Vamos simular o treinamento do seu Transformer e comparar seus resultados!")

    with st.expander("ü§î Explicar de forma simples: O que √© BLEU score e por que ele √© importante?"):
        st.write("O BLEU (Bilingual Evaluation Understudy) score √© uma m√©trica muito comum usada para avaliar a qualidade de uma tradu√ß√£o autom√°tica. Basicamente, ele compara a tradu√ß√£o gerada pelo seu modelo com uma ou mais 'tradu√ß√µes de refer√™ncia' (tradu√ß√µes de alta qualidade feitas por humanos).")
        st.write("Quanto mais a tradu√ß√£o do seu modelo se sobrep√µe e se parece com as tradu√ß√µes de refer√™ncia (considerando palavras, pares de palavras, etc.), maior ser√° o BLEU score. Um BLEU score mais alto geralmente significa que a tradu√ß√£o gerada pelo modelo √© de melhor qualidade e mais fluida.")

    st.subheader("Simulando Treinamento... ‚è≥")
    progress_bar = st.progress(0)
    for i in range(100):
        time.sleep(0.02) # Simula o tempo de treinamento
        progress_bar.progress(i + 1)
    st.success("‚úÖ Treinamento Conclu√≠do! Seu Transformer est√° pronto!")

    st.write("Aqui est√£o os resultados comparativos do Transformer em tarefas de tradu√ß√£o, baseados nos dados do paper:")

    data = {
        "Model": ["ByteNet", "GNMT + RL", "ConvS2S", "Transformer (base model)", "Transformer (big)"],
        "BLEU (EN-DE)": ["23.75", "24.6", "25.16", "**27.3**", "**28.4**"],
        "Training Cost (FLOPs)": ["$2.3\\cdot10^{19}$", "$1.4\\cdot10^{21}$", "$9.6\\cdot10^{18}$", "**$3.3\\cdot10^{18}$**", "**$2.3\\cdot10^{19}$**"]
    }
    st.table(data)

    st.write("Observe como o Transformer (big) alcan√ßou um BLEU score de **28.4** no ingl√™s-para-alem√£o, superando modelos anteriores, incluindo ensembles, por mais de 2 BLEU! ****")
    st.write("E mais importante, o treinamento foi significativamente mais r√°pido e mais paralelizado! O *Transformer (base model)*, apesar de menor, j√° demonstrou um custo de treinamento (FLOPs) muito menor que os outros modelos de ponta. ****")

    st.write("Sua miss√£o est√° completa, engenheiro! Voc√™ construiu e otimizou um Transformer com sucesso!")
    if st.button("Ver Resumo das Descobertas üèÜ", key="p5_summary_button"): # Adicionado key
        print("Bot√£o 'Ver Resumo' clicado!") # Log para depura√ß√£o
        st.session_state.game_state = "summary"
        st.rerun()
    report_bug_section()


def game_summary():
    st.header("Miss√£o Conclu√≠da! Principais Descobertas do Paper 'Attention Is All You Need' üéâ")
    st.write("Parab√©ns, engenheiro! Voc√™ demonstrou uma compreens√£o profunda do **Transformer**.")
    st.subheader("Principais Temas do Artigo:")
    st.markdown("""
    * **Arquitetura Baseada Apenas em Aten√ß√£o:** O Transformer abandona completamente as redes recorrentes (RNNs) e convolucionais (CNNs), baseando-se unicamente em mecanismos de aten√ß√£o. Isso permite um paralelismo sem precedentes no treinamento.
    * **Paraleliza√ß√£o Aumentada:** A natureza n√£o sequencial do Transformer permite que o treinamento seja muito mais eficiente, com tempos de treinamento significativamente menores em compara√ß√£o com modelos anteriores.
    * **Mecanismo de Auto-Aten√ß√£o (Self-Attention):** √â o cora√ß√£o do Transformer, permitindo que o modelo relacione diferentes posi√ß√µes da mesma sequ√™ncia (ex: palavras em uma frase) para calcular uma representa√ß√£o mais rica de cada token.
    * **Aten√ß√£o Ponderada por Ponto-Produto Escalonado (Scaled Dot-Product Attention):** Uma fun√ß√£o de aten√ß√£o espec√≠fica que escala os produtos escalares por $\\sqrt{d_k}$ para evitar problemas de gradiente (gradientes muito pequenos ou muito grandes) na fun√ß√£o softmax, crucial para a estabilidade e o aprendizado do modelo.
    * **Aten√ß√£o Multi-Cabe√ßa (Multi-Head Attention):** Em vez de uma √∫nica "cabe√ßa" de aten√ß√£o, o modelo usa v√°rias, cada uma focando em diferentes "partes" da informa√ß√£o ou diferentes tipos de rela√ß√µes. Isso enriquece as representa√ß√µes aprendidas e mant√©m a efici√™ncia computacional.
    * **Codifica√ß√£o Posicional (Positional Encoding):** Essencial para injetar informa√ß√µes sobre a ordem e a posi√ß√£o das palavras na sequ√™ncia, j√° que o modelo n√£o processa as palavras sequencialmente. O paper utilizou fun√ß√µes seno e cosseno para isso, permitindo inclusive a extrapola√ß√£o para sequ√™ncias de tamanhos diferentes das vistas no treinamento.
    * **Resultados de Ponta (State-of-the-Art):** O Transformer alcan√ßou resultados superiores (BLEU score) em tarefas de tradu√ß√£o autom√°tica (WMT 2014 English-to-German e English-to-French), superando modelos anteriores com custos de treinamento menores devido √† sua alta paraleliza√ß√£o.
    * **Generaliza√ß√£o para Outras Tarefas:** O modelo demonstrou a capacidade de generalizar bem para outras tarefas de processamento de linguagem natural, como an√°lise sint√°tica (constituency parsing).
    """)
    if st.button("Jogar Novamente üîÅ", key="summary_replay_button"): # Adicionado key
        print("Bot√£o 'Jogar Novamente' clicado!") # Log para depura√ß√£o
        # Limpa o estado da sess√£o para reiniciar o jogo
        for key in st.session_state.keys():
            del st.session_state[key]
        st.rerun()
    report_bug_section()

def report_bug_section():
    st.sidebar.subheader("üêû Reportar Erro / Sugest√£o")
    # Adicionado 'clear_on_submit=True' para limpar o campo ap√≥s o envio do formul√°rio
    with st.sidebar.form("bug_report_form", clear_on_submit=True):
        # Removido 'key' do st.text_area e do st.form_submit_button, pois causavam TypeError
        bug_text = st.text_area("Descreva o erro que encontrou ou sua sugest√£o de melhoria:")
        submitted = st.form_submit_button("Enviar Feedback ‚úâÔ∏è")
        if submitted and bug_text:
            log_feedback(bug_text)
        elif submitted and not bug_text:
            st.sidebar.warning("Por favor, escreva algo antes de enviar.")

# --- Controle do Estado do Jogo ---
# Inicializa as vari√°veis de estado do jogo, se ainda n√£o existirem
if 'game_state' not in st.session_state:
    st.session_state.game_state = "menu"
# Inicializa as tentativas de cada fase para evitar KeyError (garantindo que existam sempre)
if 'p1_attempts' not in st.session_state: st.session_state.p1_attempts = 0
if 'p2_attempts' not in st.session_state: st.session_state.p2_attempts = 0
if 'p3_attempts' not in st.session_state: st.session_state.p3_attempts = 0
if 'p4_attempts' not in st.session_state: st.session_state.p4_attempts = 0


# Renderiza a fase do jogo com base no estado atual
print(f"Estado do jogo atual: {st.session_state.game_state}") # Log para depura√ß√£o
if st.session_state.game_state == "menu":
    main_menu()
elif st.session_state.game_state == "phase1":
    phase1_architecture()
elif st.session_state.game_state == "phase2":
    phase2_scaled_dot_product_attention()
elif st.session_state.game_state == "phase3":
    phase3_multi_head_attention()
elif st.session_state.game_state == "phase4":
    phase4_positional_encoding()
elif st.session_state.game_state == "phase5":
    phase5_training_results()
elif st.session_state.game_state == "summary":
    game_summary()
