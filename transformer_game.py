import streamlit as st
import time
import datetime
import os
import numpy as np
import matplotlib.pyplot as plt
import requests

# --- Inicializa√ß√£o de Estado ---
def init_state():
    st.session_state.setdefault("game_state", "menu")
    for key in [
        "phase1_passed", "phase2_passed", "phase3_passed", "phase4_passed",
        "show_phase1_feedback", "show_phase2_feedback", "show_phase3_feedback", "show_phase4_feedback"
    ]:
        st.session_state.setdefault(key, False)
    for key in ["p1_attempts", "p2_attempts", "p3_attempts", "p4_attempts"]:
        st.session_state.setdefault(key, 0)

init_state()

# --- Logs de Feedback ---
from github import Github
import datetime
import streamlit as st

def log_feedback(feedback_text):
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    full_text = f"[{timestamp}] {feedback_text}\n"

    token = st.secrets["GITHUB_TOKEN"]
    repo_name = st.secrets["REPO_NAME"]
    file_path = st.secrets["FILE_PATH"]

    g = Github(token)
    repo = g.get_repo(repo_name)

    try:
        contents = repo.get_contents(file_path)
        new_content = contents.decoded_content.decode() + full_text
        repo.update_file(file_path, "append feedback", new_content, contents.sha)
    except Exception:
        repo.create_file(file_path, "create feedback log", full_text)

    st.success("‚úÖ Feedback salvo com sucesso no reposit√≥rio privado!")

# --- Fun√ß√£o lateral de bug/sugest√£o ---
def report_bug_section():
    st.sidebar.subheader("üêû Reportar Erro Conceitual do Jogo")
    with st.sidebar.form("bug_report_form", clear_on_submit=True):
        bug_text = st.text_area("Descreva o erro que encontrou ou sua sugest√£o de melhoria:")
        submitted = st.form_submit_button("Enviar Feedback ‚úâÔ∏è")
        if submitted:
            if bug_text.strip():
                log_feedback(bug_text)
            else:
                st.sidebar.warning("Por favor, escreva algo antes de enviar.")

# --- Fun√ß√£o lateral de llms ---

from huggingface_hub import InferenceClient

import requests

import requests

def llm_sidebar_consultation():
    st.sidebar.subheader("ü§ñ Tem alguma d√∫vida? Pergunte aqui para a LLM! (Qwen2.5-7B-Instruct, via Hugging Face)")
    user_question = st.sidebar.text_area("Digite sua d√∫vida abaixo:", key="hf_chat_user_question")

    if st.sidebar.button("Enviar pergunta", key="hf_chat_submit") and user_question.strip():
        with st.spinner("Consultando a LLM..."):
            try:
                hf_token = st.secrets["HF_TOKEN"]
                headers = {
                    "Authorization": f"Bearer {hf_token}",
                    "Content-Type": "application/json"
                }

                API_URL = "https://router.huggingface.co/together/v1/chat/completions"
                payload = {
                    "model": "Qwen/Qwen2.5-7B-Instruct-Turbo",
                    "messages": [
                        {"role": "user", "content": user_question.strip()}
                    ],
                    "temperature": 0.7,
                    "max_tokens": 300
                }

                response = requests.post(API_URL, headers=headers, json=payload, timeout=30)

                if response.status_code == 200:
                    result = response.json()
                    reply = result["choices"][0]["message"]["content"]
                    st.sidebar.success("üìò Resposta da LLM:")
                    st.sidebar.markdown(f"> {reply.strip()}")
                elif response.status_code == 429:
                    st.sidebar.error("‚ö†Ô∏è Ops, atingimos o limite de requests para o modelo!")
                else:
                    st.sidebar.error("‚ùå Ocorreu um erro inesperado ao consultar a LLM.")

            except Exception as e:
                st.sidebar.error("‚ùå Ocorreu um erro t√©cnico ao tentar se conectar √† LLM.")

    # üîΩ Adiciona separador entre a LLM e a caixa de feedback de erro conceitual
    st.sidebar.markdown("---")


# --- Fase 1: Mini-game de Montagem do Transformer ---
def phase1_architecture():
    st.header("Fase 1: A Arquitetura Fundacional (Encoder-Decoder) üèóÔ∏è")

    st.markdown("""
> üìò **Conceito-chave do artigo**  
> "Nosso modelo segue a arquitetura geral do transformador como uma pilha de camadas de codificador e decodificador."  
> ‚Äî *Vaswani et al., 2017*

A arquitetura Encoder-Decoder permite que o modelo processe a entrada por completo antes de gerar a sa√≠da, otimizando tarefas como tradu√ß√£o, resumo e question answering.
    """)

    st.write("Arraste os blocos abaixo para a ordem correta da arquitetura Transformer: da entrada at√© a sa√≠da.")

    componentes = [
        "Mecanismo de Aten√ß√£o",
        "Camada de Sa√≠da",
        "Decoder",
        "Encoder",
        "Embedding"
    ]

    ordem_correta = [
        "Embedding", "Encoder", "Mecanismo de Aten√ß√£o", "Decoder", "Camada de Sa√≠da"
    ]

    dicas = [
        "Posi√ß√£o 1 - **Embedding**: transforma cada palavra em um vetor num√©rico compreens√≠vel pela IA.",
        "Posi√ß√£o 2 - **Encoder**: processa a frase de entrada e gera uma representa√ß√£o contextualizada.",
        "Posi√ß√£o 3 - **Mecanismo de Aten√ß√£o**: decide quais palavras s√£o mais importantes umas para as outras.",
        "Posi√ß√£o 4 - **Decoder**: gera a frase de sa√≠da, com base na aten√ß√£o e no encoder.",
        "Posi√ß√£o 5 - **Camada de Sa√≠da**: traduz a sa√≠da do decoder para palavras compreens√≠veis."
    ]

    escolhas = []
    for i in range(len(ordem_correta)):
        st.markdown(dicas[i])
        escolha = st.selectbox(f"Escolha para a posi√ß√£o {i + 1}", ["‚¨áÔ∏è Escolha"] + componentes, key=f"fase1_{i}")
        escolhas.append(escolha)

    if st.button("Verificar Ordem"):
        if escolhas == ordem_correta:
            st.session_state.phase1_passed = True
            st.session_state.show_phase1_feedback = True
            st.rerun()
        else:
            st.error("‚ùå Ainda n√£o est√° certo! Tente organizar os blocos na sequ√™ncia l√≥gica.")

    if st.session_state.get("show_phase1_feedback", False):
        st.success("‚úÖ Correto! Essa √© a ordem de processamento do Transformer.")
        st.image("img/transformer.png", width=300, caption="Arquitetura do Transformer: Encoder-Decoder com Aten√ß√£o")
        if st.button("Avan√ßar para Fase 2 ‚û°Ô∏è", key="p1_advance_button"):
            st.session_state.game_state = "phase2"
            st.session_state.show_phase1_feedback = False
            st.rerun()

    st.markdown("""
> üî¨ **Al√©m do artigo**  
> Modelos como **T5**, **BART** e muitos sistemas modernos de tradu√ß√£o neural usam variantes dessa arquitetura.  
> A separa√ß√£o clara entre codifica√ß√£o e decodifica√ß√£o facilita o **aprendizado transferido (transfer learning)**, a modulariza√ß√£o e a adapta√ß√£o para tarefas distintas ‚Äî como sumariza√ß√£o, di√°logo e at√© gera√ß√£o de c√≥digo.
    """)

    llm_sidebar_consultation()
    report_bug_section()


# --- Fase 2 ---
def phase2_scaled_dot_product_attention():
    st.header("Fase 2: Corrida de Vetores e Escalonamento üéØ")

    st.markdown("""
> üìò **Conceito-chave do artigo**  
> "Utilizamos aten√ß√£o por produto escalar escalonado, que √© r√°pida e eficiente em termos de espa√ßo computacional."  
> ‚Äî *Vaswani et al., 2017*

A divis√£o por ‚àöd‚Çñ evita que os valores da softmax se tornem extremos, preservando gradientes √∫teis para aprendizado. Essa opera√ß√£o √© fundamental para a estabilidade da rede durante o treinamento.
    """)

    with st.expander("ü§î O que s√£o Q, K e d‚Çñ?"):
        st.markdown("""
- **Q (Query - Consulta):** Representa o vetor da palavra que est√° buscando contexto.  
- **K (Key - Chave):** Representa as palavras candidatas a fornecer esse contexto.  
- **d‚Çñ (dimens√£o da chave):** Tamanho dos vetores Q e K.  
- Se d‚Çñ for grande, os produtos Q¬∑K podem saturar a softmax. Por isso escalonamos.
        """)
        st.markdown("A f√≥rmula da aten√ß√£o √©:")
        st.latex(r"Attention(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V")

    q_val = st.slider("Valor do vetor Q (intensidade da consulta)", 1, 100, 60, step=1)
    k_val = st.slider("Valor do vetor K (intensidade da chave)", 1, 100, 80, step=1)
    d_k = st.slider("Dimens√£o d‚Çñ (tamanho do vetor)", 1, 128, 64, step=1)

    produto = q_val * k_val
    com_escalonamento = produto / (d_k ** 0.5)

    st.markdown(f"**Produto Escalar (Q¬∑K):** `{produto}`")
    st.markdown(f"**Com Escalonamento (√∑ ‚àöd‚Çñ):** `{com_escalonamento:.2f}`")

    if 10 <= com_escalonamento <= 30:
        st.success("‚úÖ Excelente! O valor escalonado est√° em uma faixa ideal para o funcionamento do softmax.")
        st.info("üìò Dica: valores entre **10 e 30** mant√™m a softmax balanceada e os gradientes √∫teis.")
        if st.button("Avan√ßar para Fase 3 ‚û°Ô∏è", key="p2_advance_button"):
            st.session_state.game_state = "phase3"
            st.rerun()
    else:
        st.warning("‚ö†Ô∏è O valor escalonado ainda est√° fora do ideal. Tente ajustar Q, K ou d‚Çñ para obter resultado entre **10 e 30**.")

    st.markdown("""
> üî¨ **Al√©m do artigo**  
> A dimens√£o dos vetores **Q e K** afeta a expressividade da aten√ß√£o:  
> - Vetores **pequenos** (ex: 16, 32) n√£o capturam nuances complexas.  
> - Vetores **grandes demais** (ex: 128, 256) causam produtos exagerados ‚Üí satura√ß√£o da softmax ‚Üí aprendizado prejudicado.  
>  
> A escalagem por ‚àöd‚Çñ **compensa esse efeito**, mantendo os gradientes est√°veis.  
>  
> Na pr√°tica, isso √© essencial em **modelos como GPT ou T5**, que processam sequ√™ncias longas e dependem de uma aten√ß√£o est√°vel para manter coer√™ncia sem degradar o aprendizado em passos distantes.
    """)

    llm_sidebar_consultation()
    report_bug_section()

# --- Fase 3 ---
def phase3_multi_head_attention():
    st.header("Fase 3: Multi-Head Attention: Cabe√ßas Paralelas üß†")

    st.markdown("""
> üìò **Conceito-chave do artigo**  
> "Ao inv√©s de uma √∫nica aten√ß√£o com vetores de dimens√£o d‚Çò‚Çíd‚Çë‚Çó, projetamos Q, K, V m√∫ltiplas vezes (h cabe√ßas) para subespa√ßos menores, permitindo que o modelo atenda simultaneamente a diferentes informa√ß√µes de diferentes posi√ß√µes."  
> ‚Äî *Vaswani et al., 2017*

A Multi-Head Attention permite que o Transformer olhe para a mesma informa√ß√£o de diversas maneiras simultaneamente, aprendendo padr√µes variados entre tokens.
    """)

    frase = ["O", "modelo", "aprende", "rela√ß√µes", "entre", "tokens"]
    st.write("Escolha uma palavra para observar como diferentes cabe√ßas podem reagir a ela:")

    foco = st.selectbox("Palavra de foco (query)", frase, key="p3_query")

    padroes_cabeca1 = {
        "O": ["modelo"],
        "modelo": ["O", "aprende"],
        "aprende": ["modelo"],
        "rela√ß√µes": ["entre"],
        "entre": ["rela√ß√µes"],
        "tokens": ["entre"]
    }

    padroes_cabeca2 = {
        "O": ["aprende"],
        "modelo": ["rela√ß√µes"],
        "aprende": ["tokens"],
        "rela√ß√µes": ["modelo"],
        "entre": ["aprende"],
        "tokens": ["O"]
    }

    padroes_cabeca3 = {
        "O": ["O"],
        "modelo": ["tokens"],
        "aprende": ["rela√ß√µes"],
        "rela√ß√µes": ["tokens"],
        "entre": ["modelo"],
        "tokens": ["rela√ß√µes"]
    }

    st.markdown("üîé **Cabe√ßa 1** (posi√ß√£o local): tende a olhar para palavras vizinhas.")
    st.markdown("üîé **Cabe√ßa 2** (liga√ß√£o estrutural): conecta palavras com depend√™ncia gramatical.")
    st.markdown("üîé **Cabe√ßa 3** (sem√¢ntica impl√≠cita): foca em termos semanticamente relacionados.")

    st.markdown("---")
    st.markdown(f"üß† Com foco em **{foco}**, veja como cada cabe√ßa pode responder:")

    st.write(f"**Cabe√ßa 1:** Aten√ß√£o distribu√≠da para: {', '.join(padroes_cabeca1.get(foco, []))}")
    st.write(f"**Cabe√ßa 2:** Aten√ß√£o distribu√≠da para: {', '.join(padroes_cabeca2.get(foco, []))}")
    st.write(f"**Cabe√ßa 3:** Aten√ß√£o distribu√≠da para: {', '.join(padroes_cabeca3.get(foco, []))}")

    st.success("‚úÖ Observe como diferentes cabe√ßas focam em padr√µes distintos ‚Äî essa diversidade √© essencial para que o modelo compreenda m√∫ltiplas rela√ß√µes contextuais ao mesmo tempo.")

    if st.button("Avan√ßar para Fase 4 ‚û°Ô∏è", key="p3_advance_button"):
        st.session_state.game_state = "phase4"
        st.rerun()

    st.markdown("""
> üî¨ **Al√©m do artigo**  
> Em modelos maiores como **GPT-3 ou PaLM**, o n√∫mero de cabe√ßas cresce (ex: 96 ou mais).  
> Cada uma aprende de forma independente:  
> - Algumas especializam-se em pontua√ß√£o, outras em coes√£o, ou em longas depend√™ncias sint√°ticas.  
> - A diversidade entre cabe√ßas √© essencial para tarefas como sumariza√ß√£o, programa√ß√£o, tradu√ß√£o ou racioc√≠nio matem√°tico.  
>  
> Mesmo cabe√ßas com desempenho fraco isoladamente podem ser √∫teis dentro do conjunto.
    """)

    llm_sidebar_consultation()
    report_bug_section()

# --- Fase 4 ---
import numpy as np
import matplotlib.pyplot as plt

def phase4_positional_encoding():
    st.header("Fase 4: Codifica√ß√£o Posicional (Positional Encoding) üåê")

    st.markdown("""
> üìò **Conceito-chave do artigo**  
> ‚ÄúComo o modelo n√£o possui mecanismos recorrentes ou convolucionais, √© necess√°rio incorporar alguma informa√ß√£o sobre a ordem das palavras na sequ√™ncia. Para isso, usamos fun√ß√µes senoidais que variam com a posi√ß√£o.‚Äù  
> ‚Äî *Vaswani et al., 2017*

Transformers n√£o t√™m no√ß√£o da ordem dos tokens por padr√£o. Para isso, adicionam aos embeddings vetores de **codifica√ß√£o posicional** ‚Äî combina√ß√µes de seno e cosseno ‚Äî que representam a posi√ß√£o de cada palavra na sequ√™ncia.

Essas fun√ß√µes produzem padr√µes cont√≠nuos e diferenci√°veis, permitindo que o modelo:
- Reconhe√ßa a **posi√ß√£o absoluta** dos tokens
- Codifique **rela√ß√µes de dist√¢ncia** entre palavras
- **Extrapole** para comprimentos de sequ√™ncia maiores que os vistos no treino
""")

    st.subheader("üî¢ Visualiza√ß√£o: Senoides para representar posi√ß√µes")

    posicoes = np.arange(0, 50)
    dim = 16  # Exemplo com 16 dimens√µes
    pos_enc = np.array([
        [np.sin(p / (10000 ** (2 * i / dim))) if i % 2 == 0 else np.cos(p / (10000 ** (2 * (i - 1) / dim))) for i in range(dim)]
        for p in posicoes
    ])

    fig, ax = plt.subplots(figsize=(8, 3))
    ax.plot(posicoes, pos_enc[:, 0], label="Dimens√£o 0 (seno)")
    ax.plot(posicoes, pos_enc[:, 1], label="Dimens√£o 1 (cosseno)")
    ax.set_title("Varia√ß√£o senoidal em duas dimens√µes do Positional Encoding")
    ax.set_xlabel("Posi√ß√£o do token")
    ax.legend()
    st.pyplot(fig)

    st.markdown("Acima, vemos como diferentes dimens√µes oscilam de forma distinta conforme a posi√ß√£o muda. Isso cria um **padr√£o √∫nico** por posi√ß√£o, que pode ser aprendido pelo modelo.")

    st.subheader("üß† Pergunta")
    resposta = st.radio("O que o Positional Encoding permite ao Transformer?", [
        "Capturar a import√¢ncia sem√¢ntica das palavras",
        "Aprender a ordem e a dist√¢ncia entre os tokens",
        "Entender a frequ√™ncia de cada palavra",
        "Ignorar a posi√ß√£o, j√° que a aten√ß√£o cuida disso"
    ], index=0, key="fase4_radio")

    if resposta:
        if resposta == "Aprender a ordem e a dist√¢ncia entre os tokens":
            st.success("‚úÖ Correto! O Positional Encoding insere padr√µes que permitem ao modelo saber quem vem antes ou depois, e qu√£o longe cada palavra est√° da outra.")
            if st.button("Avan√ßar para Fase 5 ‚û°Ô∏è", key="p4_advance_button"):
                st.session_state.game_state = "phase5"
                st.rerun()
        else:
            st.error("‚ùå Ainda n√£o! Lembre-se: o objetivo do Positional Encoding √© oferecer ao modelo uma forma de representar a **ordem e dist√¢ncia** entre tokens ‚Äî algo que, sozinho, a aten√ß√£o n√£o captura.")

    st.markdown("""
> üî¨ **Al√©m do artigo**  
> Muitos modelos modernos (como BERT e GPT) usam variantes de codifica√ß√£o posicional:  
> - **Fixas** (como seno/cosseno) ‚Üí extrapolam para posi√ß√µes al√©m do treino  
> - **Aprendidas** ‚Üí mais flex√≠veis, mas menos interpret√°veis  
>  
> A codifica√ß√£o posicional continua sendo uma das maiores inova√ß√µes dos Transformers ‚Äî e uma das raz√µes para sua escalabilidade.
    """)

    llm_sidebar_consultation()
    report_bug_section()


# --- Fase 5 ---
def phase5_training_results():
    st.header("Fase 5: Treinamento e Otimiza√ß√£o (Resultados e Efici√™ncia) ‚ö°")

    st.markdown("""
> üìò **Conceito-chave do artigo**  
> "O modelo Transformer atinge resultados de ponta em tradu√ß√£o autom√°tica, com menor custo computacional de treinamento comparado a modelos anteriores."  
> ‚Äî *Vaswani et al., 2017*

A arquitetura baseada em aten√ß√£o pura permite paralelismo eficiente e melhora a escalabilidade, reduzindo o tempo e custo de treinamento mesmo com grande volume de dados.
    """)

    st.subheader("Simulando Treinamento... ‚è≥")
    progress_bar = st.progress(0)
    for i in range(100):
        time.sleep(0.01)
        progress_bar.progress(i + 1)
    st.success("‚úÖ Treinamento Conclu√≠do! Seu Transformer est√° pronto!")

    st.write("Aqui est√£o os resultados comparativos do Transformer em tarefas de tradu√ß√£o (WMT 2014 EN-DE):")

    st.markdown("""
**Legenda:**
- üü¢ BLEU Score: quanto mais alto, melhor
- üîµ FLOPs (Floating Point Operations): quanto menor, mais eficiente
    """)

    data = {
        "Modelo": ["ByteNet", "GNMT + RL", "ConvS2S", "Transformer (base)", "Transformer (big)"],
        "BLEU (EN-DE)": ["23.75", "24.6", "25.16", "**27.3** üü¢", "**28.4** üü¢"],
        "Custo de Treinamento (FLOPs)": ["$2.3\\cdot10^{19}$", "$1.4\\cdot10^{21}$", "$9.6\\cdot10^{18}$", "**$3.3\\cdot10^{18}$** üîµ", "**$2.3\\cdot10^{19}$**"]
    }
    st.table(data)

    # üîù Mini ranking
    if st.button("üîù Destacar o melhor modelo"):
        st.info("üèÜ **Transformer (big)** se destaca com **BLEU 28.4** e excelente desempenho em tradu√ß√£o autom√°tica!")

    st.markdown("""
> üî¨ **Al√©m do artigo**  
> O BLEU Score √© uma m√©trica baseada em n-gramas que compara a sa√≠da gerada com tradu√ß√µes humanas.  
> - Um aumento de **2 BLEU** pode representar uma diferen√ßa **percept√≠vel na flu√™ncia e precis√£o**.  
> - O Transformer n√£o s√≥ superou modelos anteriores, mas o fez com muito **menos custo de FLOPs**.  

Isso abriu caminho para aplica√ß√µes em tempo real, como tradu√ß√£o simult√¢nea, assistentes virtuais multil√≠ngues e at√© gera√ß√£o de c√≥digo (com adapta√ß√µes).
    """)

    st.success("üöÄ Sua miss√£o foi cumprida com sucesso: voc√™ treinou um Transformer de ponta!")

    if st.button("Ver Resumo Final üèÜ", key="p5_summary_button"):
        st.session_state.game_state = "summary"
        st.rerun()

    llm_sidebar_consultation()
    report_bug_section()


# --- Resumo Final + LLM ---
def game_summary():
    st.header("Miss√£o Conclu√≠da! Recapitula√ß√£o do artigo 'Attention Is All You Need' üéâ")
    st.subheader("üß† Voc√™ demonstrou uma compreens√£o s√≥lida dos fundamentos do Transformer!")

    st.markdown("""
> üìò **Conceito central do artigo**  
> "A arquitetura Transformer depende exclusivamente de mecanismos de aten√ß√£o, eliminando o uso de recorr√™ncia e convolu√ß√£o, permitindo paraleliza√ß√£o eficiente."  
> ‚Äî *Vaswani et al., 2017*
    """)

    st.markdown("### üß© Elementos centrais explorados no jogo")

    st.markdown("""
#### 1. **Arquitetura Encoder-Decoder baseada em aten√ß√£o**
- O modelo √© organizado em **camadas empilhadas** de codificadores e decodificadores.
- O **Encoder** transforma a entrada em uma representa√ß√£o contextual.
- O **Decoder** gera a sa√≠da com base nessa representa√ß√£o e nas posi√ß√µes anteriores.
- Isso permite lidar com **tarefas de tradu√ß√£o**, sumariza√ß√£o e outras sequenciais com alta flexibilidade.
""")

    st.markdown("""
#### 2. **Mecanismo de Aten√ß√£o por Produto Escalar Escalonado**
- A aten√ß√£o compara a *query* com todas as *keys* e pondera os *values*.
- O produto Q¬∑K √© **escalonado por ‚àöd‚Çñ**, evitando satura√ß√£o da fun√ß√£o softmax.
- Isso mant√©m os **gradientes √∫teis** e o **treinamento est√°vel**, mesmo em modelos grandes.
""")

    st.markdown("""
#### 3. **Aten√ß√£o Multi-Cabe√ßa (Multi-Head Attention)**
- Em vez de uma √∫nica aten√ß√£o, o modelo usa m√∫ltiplas cabe√ßas independentes.
- Cada cabe√ßa aprende um padr√£o diferente: **estrutura, sem√¢ntica, posi√ß√£o, depend√™ncias**.
- No final, os resultados s√£o **concatenados** e projetados novamente, enriquecendo a representa√ß√£o.
""")

    st.markdown("""
#### 4. **Positional Encoding**
- Como o Transformer **n√£o possui recorr√™ncia**, ele precisa saber a posi√ß√£o das palavras.
- Usando **fun√ß√µes seno e cosseno**, cada posi√ß√£o recebe uma curva √∫nica, cont√≠nua e extrapol√°vel.
- Isso permite ao modelo lidar com **ordem das palavras** mesmo em contextos longos ou fora da distribui√ß√£o.
""")

    st.markdown("""
#### 5. **Efici√™ncia de Treinamento e Resultados**
- O Transformer atinge **BLEU scores superiores** a modelos anteriores com **menos FLOPs**.
- A aus√™ncia de recorr√™ncia permite **paraleliza√ß√£o total** no treinamento.
- Sua efici√™ncia abriu caminho para modelos massivos como BERT, GPT, T5, e muitos outros.
""")

    st.markdown("### üåç Impactos no mundo real")
    st.markdown("""
- Permitiu o surgimento de modelos de linguagem de c√≥digo aberto e escal√°veis.
- Influenciou modelos em **√°udio, vis√£o computacional, bioinform√°tica e rob√≥tica**.
- Tornou poss√≠vel o treinamento em **paralelo em GPUs e TPUs**, reduzindo drasticamente o tempo de infer√™ncia.

> üî¨ O Transformer mudou profundamente o paradigma de modelagem de linguagem ‚Äî e sua miss√£o hoje mostra que voc√™ compreende as engrenagens por tr√°s dessa revolu√ß√£o.
    """)

    if st.button("Jogar novamente üîÅ", key="summary_replay_button"):
        for key in list(st.session_state.keys()):
            del st.session_state[key]
        st.rerun()

    llm_sidebar_consultation()
    report_bug_section()

# --- Menu Inicial ---
def main_menu():
    st.title("üöÄ A Jornada do Transformer: Aten√ß√£o Desvendada! üöÄ")

    st.write("""
Esse √© um jogo interativo pensado para ajudar voc√™ a revisar, de forma leve e engajada, os principais conceitos do paper cl√°ssico *Attention is All You Need*. [Leia o paper original](https://arxiv.org/abs/1706.03762).

Durante o jogo, voc√™ ser√° guiado por cinco fases, cada uma com um mini-desafio sobre aspectos fundamentais do Transformer: arquitetura, aten√ß√£o escalonada, aten√ß√£o multi-cabe√ßa, codifica√ß√£o posicional e resultados de desempenho.

No canto lateral esquerdo, voc√™ pode:
- ‚ùì Consultar uma **LLM integrada** sempre que tiver d√∫vidas sobre os conceitos apresentados.
- üêû Usar a **caixinha de feedback** para reportar erros conceituais ou sugerir melhorias a qualquer momento.
    """)

    # üîΩ Centralizar imagem com layout de colunas
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.image("img/transformer.png", use_container_width=True)

    if st.button("Iniciar Miss√£o ‚û°Ô∏è"):
        st.session_state.game_state = "phase1"
        st.rerun()
    report_bug_section()

# --- Navega√ß√£o ---
fases_nomes = {
    "menu": "Menu Inicial",
    "phase1": "Fase 1",
    "phase2": "Fase 2",
    "phase3": "Fase 3",
    "phase4": "Fase 4",
    "phase5": "Fase 5",
    "summary": "Resumo Final"
}
estado_legivel = fases_nomes.get(st.session_state.game_state, "Desconhecido")
st.write(f"üß≠ Estado atual: {estado_legivel}")

if st.session_state.game_state == "menu":
    main_menu()
elif st.session_state.game_state == "phase1":
    phase1_architecture()
elif st.session_state.game_state == "phase2":
    phase2_scaled_dot_product_attention()
elif st.session_state.game_state == "phase3":
    phase3_multi_head_attention()
elif st.session_state.game_state == "phase4":
    phase4_positional_encoding()
elif st.session_state.game_state == "phase5":
    phase5_training_results()
elif st.session_state.game_state == "summary":
    game_summary()
