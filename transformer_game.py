import streamlit as st
import time
import datetime
import os
import numpy as np
import matplotlib.pyplot as plt
import requests

# --- InicializaÃ§Ã£o de Estado ---
def init_state():
    st.session_state.setdefault("game_state", "menu")
    for key in [
        "phase1_passed", "phase2_passed", "phase3_passed", "phase4_passed",
        "show_phase1_feedback", "show_phase2_feedback", "show_phase3_feedback", "show_phase4_feedback"
    ]:
        st.session_state.setdefault(key, False)
    for key in ["p1_attempts", "p2_attempts", "p3_attempts", "p4_attempts"]:
        st.session_state.setdefault(key, 0)

init_state()

# --- Logs de Feedback ---
LOG_FILE = "game_feedback.log"

def log_feedback(feedback_text):
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    try:
        with open(LOG_FILE, "a") as f:
            f.write(f"[{timestamp}] {feedback_text}\n")
        st.success("Obrigado pelo seu feedback! Ele foi registrado.")
    except Exception as e:
        st.error(f"Erro ao salvar feedback: {e}")

# --- FunÃ§Ã£o lateral de bug/sugestÃ£o ---
def report_bug_section():
    st.sidebar.subheader("ğŸ Reportar Erro / SugestÃ£o")
    with st.sidebar.form("bug_report_form", clear_on_submit=True):
        bug_text = st.text_area("Descreva o erro que encontrou ou sua sugestÃ£o de melhoria:")
        submitted = st.form_submit_button("Enviar Feedback âœ‰ï¸")
        if submitted:
            if bug_text.strip():
                log_feedback(bug_text)
            else:
                st.sidebar.warning("Por favor, escreva algo antes de enviar.")

# --- Fase 1: Mini-game de Montagem do Transformer ---
def phase1_architecture():
    st.header("Fase 1: A Arquitetura Fundacional (Encoder-Decoder) ğŸ—ï¸")
    
    st.markdown("""
ğŸ“˜ **Conceito-chave do artigo**:
> "Nosso modelo segue a arquitetura geral do transformador como uma pilha de camadas de codificador e decodificador."  
(Vaswani et al., 2017)

A arquitetura Encoder-Decoder permite que o modelo processe a entrada por completo antes de gerar a saÃ­da, otimizando tarefas como traduÃ§Ã£o, resumo e question answering.

ğŸ”¬ **AlÃ©m do artigo**:
Modelos como T5, BART e muitos sistemas de traduÃ§Ã£o neural atuais usam variantes dessa arquitetura. Essa separaÃ§Ã£o entre codificaÃ§Ã£o e decodificaÃ§Ã£o facilita transfer learning e modularidade.
    """)

    st.write("Arraste os blocos abaixo para a ordem correta da arquitetura Transformer: de entrada atÃ© a saÃ­da.")

    componentes = [
        "Mecanismo de AtenÃ§Ã£o",
        "Camada de SaÃ­da",
        "Decoder",
        "Encoder",
        "Embedding"
    ]

    ordem_correta = [
        "Embedding", "Encoder", "Mecanismo de AtenÃ§Ã£o", "Decoder", "Camada de SaÃ­da"
    ]

    dicas = [
        "PosiÃ§Ã£o 1 - **Embedding**: transforma cada palavra em um vetor numÃ©rico compreensÃ­vel pela IA.",
        "PosiÃ§Ã£o 2 - **Encoder**: processa a frase de entrada e gera uma representaÃ§Ã£o contextualizada.",
        "PosiÃ§Ã£o 3 - **Mecanismo de AtenÃ§Ã£o**: decide quais palavras sÃ£o mais importantes umas para as outras.",
        "PosiÃ§Ã£o 4 - **Decoder**: gera a frase de saÃ­da, com base na atenÃ§Ã£o e no encoder.",
        "PosiÃ§Ã£o 5 - **Camada de SaÃ­da**: traduz a saÃ­da do decoder para palavras compreensÃ­veis."
    ]

    escolhas = []
    for i in range(len(ordem_correta)):
        st.markdown(dicas[i])
        escolha = st.selectbox(f"Escolha para a posiÃ§Ã£o {i + 1}", ["â¬‡ï¸ Escolha"] + componentes, key=f"fase1_{i}")
        escolhas.append(escolha)

    if st.button("Verificar Ordem"):
        if escolhas == ordem_correta:
            st.session_state.phase1_passed = True
            st.session_state.show_phase1_feedback = True
            st.rerun()
        else:
            st.error("âŒ Ainda nÃ£o estÃ¡ certo! Tente organizar os blocos na sequÃªncia lÃ³gica.")

    if st.session_state.get("show_phase1_feedback", False):
        st.success("âœ… Correto! Essa Ã© a ordem de processamento do Transformer.")
        st.image("img/transformer.png", width=300, caption="Arquitetura do Transformer: Encoder-Decoder com AtenÃ§Ã£o")
        if st.button("AvanÃ§ar para Fase 2 â¡ï¸", key="p1_advance_button"):
            st.session_state.game_state = "phase2"
            st.session_state.show_phase1_feedback = False
            st.rerun()

    report_bug_section()

# --- Fase 2 ---
def phase2_scaled_dot_product_attention():
    st.header("Fase 2: Corrida de Vetores e Escalonamento ğŸ¯")

    st.markdown("""
ğŸ“˜ **Conceito-chave do artigo**:
> "Utilizamos atenÃ§Ã£o por produto escalar escalonado, que Ã© rÃ¡pida e eficiente em termos de espaÃ§o computacional."  
(Vaswani et al., 2017)

A divisÃ£o por âˆšdâ‚– evita que os valores da softmax se tornem extremos, preservando gradientes Ãºteis para aprendizado.

ğŸ”¬ **AlÃ©m do artigo**:
A atenÃ§Ã£o escalonada Ã© essencial para o bom funcionamento de grandes modelos como GPT, T5 e BERT. Pequenas variaÃ§Ãµes nesse cÃ¡lculo afetam o desempenho e a velocidade de convergÃªncia â€” especialmente em tarefas com sentenÃ§as longas ou contexto complexo.
    """)

    with st.expander("ğŸ¤” O que sÃ£o Q, K e dâ‚–?"):
        st.markdown("""
- **Q (Query - Consulta)**: representa o vetor da palavra que estÃ¡ pedindo informaÃ§Ã£o. Ele pergunta: â€œquais palavras sÃ£o relevantes para mim?â€
- **K (Key - Chave)**: representa cada uma das outras palavras que podem ser relevantes.
- **dâ‚– (dimensÃ£o da chave)**: controla o tamanho dos vetores Q e K. Serve para normalizar o cÃ¡lculo de similaridade.

O cÃ¡lculo da atenÃ§Ã£o Ã© feito assim:

```Attention(Q, K, V) = softmax(QÂ·Káµ— / âˆšdâ‚–)Â·V```

Se QÂ·K for muito grande, a softmax se satura e os gradientes viram quase zero. A divisÃ£o por âˆšdâ‚– evita isso.
        """)

    q_val = st.slider("Valor do vetor Q (intensidade da consulta)", 1, 100, 60, step=1)
    k_val = st.slider("Valor do vetor K (intensidade da chave)", 1, 100, 80, step=1)
    d_k = st.slider("DimensÃ£o dâ‚– (escalonador, tamanho dos vetores)", 1, 100, 64, step=1)

    produto = q_val * k_val
    com_escalonamento = produto / (d_k ** 0.5)

    st.markdown(f"**Produto Escalar (QÂ·K):** `{produto}`")
    st.markdown(f"**Escalonado (Ã· âˆšdâ‚–):** `{com_escalonamento:.2f}`")

    if 10 <= com_escalonamento <= 30:
        st.success("âœ… Muito bem! O valor escalonado estÃ¡ em uma faixa ideal para o funcionamento do softmax.")
        st.info("ğŸ“˜ Dica: valores escalonados entre **10 e 30** mantÃªm a softmax funcionando bem: os pesos nÃ£o ficam extremos e o modelo consegue aprender com eficiÃªncia.")
        if st.button("AvanÃ§ar para Fase 3 â¡ï¸", key="p2_advance_button"):
            st.session_state.game_state = "phase3"
            st.rerun()
    else:
        st.warning("âš ï¸ O valor escalonado ainda estÃ¡ fora do ideal. Tente ajustar Q, K ou aumentar dâ‚– para que o resultado fique entre **10 e 30**.")

    report_bug_section()

# --- Fase 3 ---
def phase3_multi_head_attention():
    st.header("Fase 3: Multi-Head Attention: CabeÃ§as Paralelas ğŸ§ ")

    st.markdown("""
ğŸ“˜ **Conceito-chave do artigo**:
> "Ã‰ benÃ©fico projetar Q, K, V h vezes com projeÃ§Ãµes lineares diferentes aprendidas, permitindo que o modelo atenda conjuntamente a informaÃ§Ãµes de diferentes subespaÃ§os de representaÃ§Ã£o em diferentes posiÃ§Ãµes."  
(Vaswani et al., 2017)

Cada cabeÃ§a de atenÃ§Ã£o aprende padrÃµes diferentes â€” como ligaÃ§Ãµes sintÃ¡ticas, proximidade posicional ou coocorrÃªncia semÃ¢ntica â€” sem que essas categorias sejam prÃ©-definidas.

ğŸ”¬ **AlÃ©m do artigo**:
Essa ideia inspirou arquiteturas como BERT e GPT, onde mÃºltiplas cabeÃ§as permitem capturar nuances finas de contexto, ironia, ambiguidade, e relaÃ§Ãµes de dependÃªncia a longa distÃ¢ncia. Em tarefas como sumarizaÃ§Ã£o, traduÃ§Ã£o e resposta automÃ¡tica, essa diversidade de atenÃ§Ã£o melhora muito a performance.
    """)

    st.subheader("Mini-visualizaÃ§Ã£o: como mÃºltiplas cabeÃ§as se comportam")
    frase = ["O", "modelo", "aprende", "relaÃ§Ãµes", "entre", "tokens"]
    st.write("Escolha uma palavra para observar como diferentes cabeÃ§as podem reagir a ela:")

    foco = st.selectbox("Palavra de foco (query)", frase, key="p3_query")

    padroes_cabeca1 = {
        "O": ["modelo"],
        "modelo": ["O", "aprende"],
        "aprende": ["modelo"],
        "relaÃ§Ãµes": ["entre"],
        "entre": ["relaÃ§Ãµes"],
        "tokens": ["entre"]
    }

    padroes_cabeca2 = {
        "O": ["aprende"],
        "modelo": ["relaÃ§Ãµes"],
        "aprende": ["tokens"],
        "relaÃ§Ãµes": ["modelo"],
        "entre": ["aprende"],
        "tokens": ["O"]
    }

    padroes_cabeca3 = {
        "O": ["O"],
        "modelo": ["tokens"],
        "aprende": ["relaÃ§Ãµes"],
        "relaÃ§Ãµes": ["tokens"],
        "entre": ["modelo"],
        "tokens": ["relaÃ§Ãµes"]
    }

    st.markdown("ğŸ” **CabeÃ§a 1** (posiÃ§Ã£o local): tende a olhar para palavras vizinhas da query.")
    st.markdown("ğŸ” **CabeÃ§a 2** (ligaÃ§Ã£o estrutural): pode conectar palavras com dependÃªncia gramatical.")
    st.markdown("ğŸ” **CabeÃ§a 3** (semÃ¢ntica implÃ­cita): pode focar em termos semanticamente relacionados.")

    st.markdown("---")
    st.markdown(f"ğŸ§  Com foco em **{foco}**, veja como cada cabeÃ§a pode responder:")

    st.write(f"**CabeÃ§a 1:** AtenÃ§Ã£o distribuÃ­da para: {', '.join(padroes_cabeca1.get(foco, [])) or 'nenhuma palavra associada'}")
    st.write(f"**CabeÃ§a 2:** AtenÃ§Ã£o distribuÃ­da para: {', '.join(padroes_cabeca2.get(foco, [])) or 'nenhuma palavra associada'}")
    st.write(f"**CabeÃ§a 3:** AtenÃ§Ã£o distribuÃ­da para: {', '.join(padroes_cabeca3.get(foco, [])) or 'nenhuma palavra associada'}")

    st.success("âœ… Observe como diferentes cabeÃ§as focam em padrÃµes distintos â€” essa diversidade Ã© fundamental para a riqueza das representaÃ§Ãµes geradas pelo Transformer.")

    if st.button("AvanÃ§ar para Fase 4 â¡ï¸", key="p3_advance_button"):
        st.session_state.game_state = "phase4"
        st.rerun()

    report_bug_section()

# --- Fase 4 ---
def phase4_positional_encoding():
    st.header("Fase 4: A ImportÃ¢ncia da PosiÃ§Ã£o (Positional Encoding) ğŸ“")

    st.markdown("""
ğŸ“˜ **Conceito-chave do artigo**:
> "Como nosso modelo nÃ£o possui nenhuma recorrÃªncia ou convoluÃ§Ã£o, adicionamos informaÃ§Ãµes de posiÃ§Ã£o Ã s embeddings de entrada em todas as camadas de codificador e decodificador."  
(Vaswani et al., 2017)

As codificaÃ§Ãµes posicionais sÃ£o baseadas em funÃ§Ãµes seno e cosseno de diferentes frequÃªncias. Isso permite ao modelo comparar posiÃ§Ãµes relativas mesmo em sequÃªncias maiores do que as vistas no treinamento.

ğŸ”¬ **AlÃ©m do artigo**:
A codificaÃ§Ã£o posicional senoidal permite que o modelo funcione mesmo em longos documentos, listas ou cÃ³digo-fonte. Em aplicaÃ§Ãµes como detecÃ§Ã£o de eventos em sÃ©ries temporais ou anÃ¡lise de DNA, a posiÃ§Ã£o relativa entre tokens Ã© fundamental.
    """)

    st.subheader("VisualizaÃ§Ã£o do caminho posicional")
    st.write("Use o controle abaixo para ajustar o tamanho da sequÃªncia (quantidade de tokens) e ver como a codificaÃ§Ã£o posicional muda:")

    num_pontos = st.slider("Tamanho da sequÃªncia (tokens)", 5, 50, 20)
    uso_seno = st.radio("Tipo de codificaÃ§Ã£o simulada:", ["Constante", "Linear", "Senoidal"], index=2)

    st.markdown("""
ğŸ” **O que muda quando vocÃª aumenta a sequÃªncia?**
- A **codificaÃ§Ã£o constante** nÃ£o representa posiÃ§Ã£o alguma.
- A **linear** sÃ³ distingue posiÃ§Ãµes por ordem direta (ex: 1, 2, 3...).
- A **senoidal**, como no artigo, permite que o modelo compare posiÃ§Ãµes relativas usando combinaÃ§Ãµes harmÃ´nicas, sendo **mais robusta e generalizÃ¡vel**.
    """)

    import numpy as np
    import matplotlib.pyplot as plt

    x = np.arange(num_pontos)
    if uso_seno == "Constante":
        y = np.ones_like(x)
    elif uso_seno == "Linear":
        y = x
    else:
        y = np.sin(x / 5)

    fig, ax = plt.subplots()
    ax.plot(x, y, marker='o')
    ax.set_title("CodificaÃ§Ã£o Posicional - VisualizaÃ§Ã£o Simulada")
    st.pyplot(fig)

    if uso_seno == "Senoidal":
        st.success("âœ… Correto! A codificaÃ§Ã£o senoidal Ã© usada no Transformer para representar posiÃ§Ã£o de forma contÃ­nua e extrapolÃ¡vel.")
        if st.button("AvanÃ§ar para Fase 5 â¡ï¸", key="p4_advance_button"):
            st.session_state.game_state = "phase5"
            st.rerun()
    else:
        st.warning("âš ï¸ A codificaÃ§Ã£o senoidal Ã© a que melhor representa a posiÃ§Ã£o, segundo o artigo. Tente selecionÃ¡-la.")

    report_bug_section()

# --- Fase 5 ---
def phase5_training_results():
    st.header("Fase 5: Treinamento e OtimizaÃ§Ã£o (Resultados e EficiÃªncia) âš™ï¸")

    st.markdown("""
ğŸ“˜ **Conceito-chave do artigo**:
> "O Transformer atinge melhores resultados com menor custo computacional comparado a arquiteturas baseadas em convoluÃ§Ãµes ou recorrÃªncia."  
(Vaswani et al., 2017)

Com um design inteiramente baseado em atenÃ§Ã£o e sem dependÃªncias sequenciais, o Transformer permite **treinamento paralelizado** e **custo reduzido**, mesmo em grandes volumes de dados.

ğŸ”¬ **AlÃ©m do artigo**:
Essa eficiÃªncia transformou o campo da IA. Modelos como GPT, T5 e BERT usam essa arquitetura para serem treinados em escala massiva com clusters de GPUs/TPUs. Ajustar o nÃºmero de cabeÃ§as, o tamanho do modelo e o batch size pode impactar significativamente o custo e a performance do sistema.
    """)

    st.subheader("Simule o Treinamento do seu Transformer")
    modelo = st.selectbox("Tamanho do modelo", ["Pequeno", "Base", "Grande"], index=1)
    num_cabecas = st.slider("NÃºmero de cabeÃ§as de atenÃ§Ã£o", 2, 16, 8)
    batch_size = st.slider("Tamanho do batch (lote)", 8, 128, 32, step=8)

    if modelo == "Pequeno":
        base_bleu = 25.0
        custo = 1.0
    elif modelo == "Base":
        base_bleu = 27.3
        custo = 2.5
    else:
        base_bleu = 28.4
        custo = 5.0

    ajuste = (num_cabecas / 8) * (batch_size / 32)
    bleu = base_bleu + np.log2(ajuste + 1)
    custo_total = custo * ajuste

    st.markdown(f"**BLEU estimado:** `{bleu:.2f}`")
    st.markdown(f"**Custo estimado (FLOPs):** `{custo_total:.2f}` unidades computacionais")

    if bleu >= 28.0:
        st.success("âœ… ParabÃ©ns! VocÃª otimizou bem seu Transformer.")
        if st.button("Ver Resumo das Descobertas ğŸ†"):
            st.session_state.game_state = "summary"
            st.rerun()
    else:
        st.warning("âš ï¸ Seu BLEU score ainda pode melhorar. Tente ajustar os parÃ¢metros!")

    report_bug_section()


# --- Resumo Final + LLM ---
def game_summary():
    st.header("Resumo: Descobertas do Artigo *Attention Is All You Need* ğŸ‰")

    st.markdown("""
VocÃª concluiu todas as fases e compreendeu os principais pilares da arquitetura Transformer. Aqui estÃ¡ um resumo aprofundado, diretamente alinhado ao artigo de Vaswani et al. (2017):

---

ğŸ“˜ **Conceitos centrais:**

1. **AtenÃ§Ã£o como mecanismo principal**  
   O Transformer substitui completamente RNNs e CNNs, baseando-se apenas em mecanismos de atenÃ§Ã£o. Isso permite maior paralelizaÃ§Ã£o e menor custo computacional.

2. **Auto-atenÃ§Ã£o (Self-Attention)**  
   Cada palavra se relaciona com todas as outras da sequÃªncia para gerar uma representaÃ§Ã£o contextualizada.

3. **Produto escalar escalonado**  
   Divide o produto QÂ·K por âˆšdâ‚– para manter os valores dentro de uma faixa Ãºtil ao softmax, evitando gradientes pequenos ou saturaÃ§Ã£o.

4. **Multi-Head Attention**  
   VÃ¡rias cabeÃ§as de atenÃ§Ã£o aprendem diferentes padrÃµes simultaneamente, enriquecendo a compreensÃ£o contextual.

5. **CodificaÃ§Ã£o Posicional Senoidal**  
   FunÃ§Ãµes seno e cosseno representam posiÃ§Ã£o dos tokens sem depender de sequÃªncia recorrente.

6. **EficiÃªncia no Treinamento**  
   O modelo alcanÃ§ou melhores resultados com menos custo, superando modelos anteriores como GNMT, ByteNet e ConvS2S.

---

ğŸ”¬ **AlÃ©m do artigo: Impacto na IA atual**

- O Transformer se tornou a base para **BERT, GPT, T5, BART, DeBERTa**, entre outros.
- Modelos baseados nele lideram benchmarks em traduÃ§Ã£o, sumarizaÃ§Ã£o, geraÃ§Ã£o de texto, classificaÃ§Ã£o, QA e mais.
- Sua estrutura modular e escalÃ¡vel possibilitou o avanÃ§o dos **LLMs (Large Language Models)** e da **IA generativa** em escala global.

""")

    st.markdown("---")
    st.subheader("â“ Pergunte sobre Transformers")
    st.write("Use o assistente abaixo para tirar dÃºvidas sobre o conteÃºdo do jogo!")

    pergunta = st.text_area("Digite sua pergunta para o modelo Falcon-7B-Instruct:", key="qa_final")
    if st.button("Responder", key="qa_submit"):
        if pergunta.strip():
            with st.spinner("Gerando resposta..."):
                resposta = requests.post(
                    "https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct",
                    json={"inputs": f"Pergunta: {pergunta}\\nResposta:"}
                )
                if resposta.status_code == 200:
                    saida = resposta.json()[0].get("generated_text", "")
                    st.markdown(saida)
                else:
                    st.error("NÃ£o foi possÃ­vel gerar uma resposta agora.")
        else:
            st.warning("Digite sua pergunta antes de enviar.")

    if st.button("Jogar Novamente ğŸ”"):
        for key in list(st.session_state.keys()):
            del st.session_state[key]
        st.rerun()

    report_bug_section()

# --- Menu Inicial ---
def main_menu():
    st.title("ğŸš€ A Jornada do Transformer: AtenÃ§Ã£o Desvendada! ğŸš€")
    st.write("Bem-vindo, engenheiro de inteligÃªncia artificial! Sua missÃ£o Ã© guiar um modelo Transformer por cinco fases...")
    try:
        st.image("img/transformer.png", width=200)
    except:
        st.warning("Imagem nÃ£o encontrada.")
    if st.button("Iniciar MissÃ£o â¡ï¸"):
        st.session_state.game_state = "phase1"
        st.rerun()
    report_bug_section()

# --- NavegaÃ§Ã£o ---
st.write(f"ğŸ§­ Estado atual: {st.session_state.game_state}")
if st.session_state.game_state == "menu":
    main_menu()
elif st.session_state.game_state == "phase1":
    phase1_architecture()
elif st.session_state.game_state == "phase2":
    phase2_scaled_dot_product_attention()
elif st.session_state.game_state == "phase3":
    phase3_multi_head_attention()
elif st.session_state.game_state == "phase4":
    phase4_positional_encoding()
elif st.session_state.game_state == "phase5":
    phase5_training_results()
elif st.session_state.game_state == "summary":
    game_summary()
