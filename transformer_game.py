import streamlit as st
import time
import datetime
import os

# --- InicializaÃ§Ã£o de Estado ---
def init_state():
    st.session_state.setdefault("game_state", "menu")
    st.session_state.setdefault("phase1_passed", False)
    st.session_state.setdefault("phase2_passed", False)
    st.session_state.setdefault("phase3_passed", False)
    st.session_state.setdefault("phase4_passed", False)
    for key in ["p1_attempts", "p2_attempts", "p3_attempts", "p4_attempts"]:
        st.session_state.setdefault(key, 0)

init_state()

# --- Logs de Feedback ---
LOG_FILE = "game_feedback.log"

def log_feedback(feedback_text):
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    try:
        with open(LOG_FILE, "a") as f:
            f.write(f"[{timestamp}] {feedback_text}\n")
        st.success("Obrigado pelo seu feedback! Ele foi registrado.")
    except Exception as e:
        st.error(f"Erro ao salvar feedback: {e}")

# --- FunÃ§Ã£o lateral de bug/sugestÃ£o ---
def report_bug_section():
    st.sidebar.subheader("ğŸ Reportar Erro / SugestÃ£o")
    with st.sidebar.form("bug_report_form", clear_on_submit=True):
        bug_text = st.text_area("Descreva o erro que encontrou ou sua sugestÃ£o de melhoria:")
        submitted = st.form_submit_button("Enviar Feedback âœ‰ï¸")
        if submitted:
            if bug_text.strip():
                log_feedback(bug_text)
            else:
                st.sidebar.warning("Por favor, escreva algo antes de enviar.")

# --- Fase 1 ---
def phase1_architecture():
    st.header("Fase 1: A Arquitetura Fundacional (Encoder-Decoder) ğŸ—ï¸")
    st.write("Antes do Transformer, os modelos mais comuns para lidar com dados sequenciais eram as redes neurais recorrentes (RNNs) e convolucionais (CNNs). No entanto, ambos possuem limitaÃ§Ãµes sÃ©rias quando se trata de capturar dependÃªncias de longo prazo em sequÃªncias e realizar o treinamento de forma paralela. O Transformer revoluciona essa abordagem ao eliminar completamente a necessidade de recorrÃªncia ou convoluÃ§Ã£o, confiando exclusivamente no mecanismo de atenÃ§Ã£o.")

    with st.expander("ğŸ¤” O que Ã© Encoder-Decoder?"):
        st.write("Essa arquitetura Ã© composta por duas partes principais: o codificador (encoder), que processa a entrada e gera uma representaÃ§Ã£o interna (vetor de contexto), e o decodificador (decoder), que utiliza essa representaÃ§Ã£o para gerar uma saÃ­da. Em tarefas como traduÃ§Ã£o automÃ¡tica, o encoder lÃª a frase em uma lÃ­ngua e o decoder gera a traduÃ§Ã£o na outra lÃ­ngua.")

    col1, col2, col3 = st.columns(3)
    with col1:
        if st.button("Redes Recorrentes Complexas ğŸ”„", key="p1_btn_rnn"):
            st.error("âŒ Resposta incorreta! As RNNs sÃ£o difÃ­ceis de treinar para sequÃªncias longas e tÃªm limitaÃ§Ãµes de paralelizaÃ§Ã£o.")
            st.session_state.p1_attempts += 1
    with col2:
        if st.button("Redes Convolucionais ğŸ–¼ï¸", key="p1_btn_cnn"):
            st.warning("âš ï¸ Parcialmente correto. As CNNs oferecem alguma paralelizaÃ§Ã£o, mas nÃ£o capturam dependÃªncias de longo prazo de maneira eficiente.")
            st.session_state.p1_attempts += 1
    with col3:
        if st.button("AtenÃ§Ã£o Pura (Transformer) âœ¨", key="p1_btn_attention"):
            st.session_state.phase1_passed = True
            st.rerun()

    if st.session_state.phase1_passed:
        st.success("âœ… Correto! O Transformer utiliza atenÃ§Ã£o pura â€” sem RNNs nem CNNs.")
        st.write("O Transformer usa pilhas de atenÃ§Ã£o e feedforward em blocos separados para codificaÃ§Ã£o e decodificaÃ§Ã£o, o que permite paralelizaÃ§Ã£o total e melhor desempenho.")
        st.image("img/transformer.png", width=300, caption="Diagrama da arquitetura do Transformer")
        if st.button("AvanÃ§ar para Fase 2 â¡ï¸", key="p1_advance_button"):
            st.session_state.game_state = "phase2"
            st.rerun()

    report_bug_section()

# --- Fase 2 ---
def phase2_scaled_dot_product_attention():
    st.header("Fase 2: AtenÃ§Ã£o Escalonada por Produto Escalar (Scaled Dot-Product Attention) ğŸ¯")
    st.write("O mecanismo de atenÃ§Ã£o calcula a relevÃ¢ncia entre elementos de uma sequÃªncia com base em vetores de consulta (query), chave (key) e valor (value). A versÃ£o escalonada melhora a estabilidade do treinamento ao dividir o produto escalar QK pela raiz quadrada da dimensÃ£o das chaves (\u221ad_k), evitando que a funÃ§Ã£o softmax sature com valores muito grandes.")

    col1, col2 = st.columns(2)
    with col1:
        if st.button("Aumentar magnitude dos produtos", key="p2_wrong"):
            st.error("âŒ Incorreto! Isso pioraria o problema da saturaÃ§Ã£o no softmax.")
            st.session_state.p2_attempts += 1
    with col2:
        if st.button("Evitar gradientes pequenos no softmax âœ…", key="p2_correct"):
            st.session_state.phase2_passed = True
            st.rerun()

    if st.session_state.phase2_passed:
        st.success("âœ… Correto! Escalar por \u221ad_k estabiliza os gradientes da atenÃ§Ã£o.")
        st.write("Isso garante que os pesos da atenÃ§Ã£o distribuÃ­dos pelo softmax fiquem em uma faixa Ãºtil para o aprendizado.")
        if st.button("AvanÃ§ar para Fase 3 â¡ï¸", key="p2_advance_button"):
            st.session_state.game_state = "phase3"
            st.rerun()

    report_bug_section()

# --- Fase 3 ---
def phase3_multi_head_attention():
    st.header("Fase 3: AtenÃ§Ã£o Multi-CabeÃ§a (Multi-Head Attention) ğŸ’¡")
    st.write("A atenÃ§Ã£o multi-cabeÃ§a divide as representaÃ§Ãµes em subespaÃ§os menores e aplica atenÃ§Ã£o separada a cada um deles. Isso permite que o modelo aprenda diferentes tipos de relacionamentos simultaneamente (por exemplo: proximidade sintÃ¡tica, associaÃ§Ã£o semÃ¢ntica etc.), melhorando a expressividade do modelo.")

    d_val = st.slider("Escolha d_k/d_v por cabeÃ§a (esperado: 64)", 32, 128, 64, step=32)
    if d_val == 64:
        st.session_state.phase3_passed = True
        st.rerun()
    elif 'p3_attempts' in st.session_state:
        st.session_state.p3_attempts += 1

    if st.session_state.phase3_passed:
        st.success("âœ… Correto! Com d_model=512 e 8 cabeÃ§as, temos d_k = d_v = 64 por cabeÃ§a.")
        st.write("Isso mantÃ©m o custo computacional comparÃ¡vel ao de uma Ãºnica cabeÃ§a com d_model completo.")
        if st.button("AvanÃ§ar para Fase 4 â¡ï¸", key="p3_advance_button"):
            st.session_state.game_state = "phase4"
            st.rerun()

    report_bug_section()

# --- Fase 4 ---
def phase4_positional_encoding():
    st.header("Fase 4: CodificaÃ§Ã£o Posicional ğŸ“")
    st.write("Como o Transformer nÃ£o possui estrutura sequencial explÃ­cita como nas RNNs, ele precisa adicionar manualmente informaÃ§Ãµes sobre a posiÃ§Ã£o dos tokens. Isso Ã© feito somando um vetor de posiÃ§Ã£o ao vetor de embedding de cada palavra. O artigo original usa funÃ§Ãµes seno e cosseno com diferentes frequÃªncias para esse fim.")

    col1, col2, col3 = st.columns(3)
    with col1:
        if st.button("Embeddings aprendidos", key="p4_wrong1"):
            st.warning("âš ï¸ MÃ©todo alternativo possÃ­vel, mas nÃ£o o utilizado no artigo original.")
            st.session_state.p4_attempts += 1
    with col2:
        if st.button("Seno e Cosseno âœ…", key="p4_correct"):
            st.session_state.phase4_passed = True
            st.rerun()
    with col3:
        if st.button("Hash de posiÃ§Ã£o", key="p4_wrong2"):
            st.error("âŒ Incorreto. Hashes nÃ£o preservam relaÃ§Ãµes posicionais.")
            st.session_state.p4_attempts += 1

    if st.session_state.phase4_passed:
        st.success("âœ… Correto! As funÃ§Ãµes trigonomÃ©tricas garantem generalizaÃ§Ã£o para sequÃªncias mais longas.")
        if st.button("AvanÃ§ar para Fase 5 â¡ï¸", key="p4_advance_button"):
            st.session_state.game_state = "phase5"
            st.rerun()

    report_bug_section()

# --- Fase 5 ---
def phase5_training_results():
    st.header("Fase 5: Treinamento e Resultados âš¡")
    st.write("Vamos simular o treinamento do Transformer e observar como ele se compara a modelos anteriores em termos de qualidade de traduÃ§Ã£o e custo computacional.")
    progress_bar = st.progress(0)
    for i in range(100):
        time.sleep(0.01)
        progress_bar.progress(i + 1)
    st.success("âœ… Treinamento concluÃ­do!")
    if st.button("Ver Resumo ğŸ†"):
        st.session_state.game_state = "summary"
        st.rerun()
    report_bug_section()

# --- Resumo Final ---
def game_summary():
    st.header("Resumo: Attention Is All You Need ğŸ‰")
    st.markdown("""
* Arquitetura baseada exclusivamente em atenÃ§Ã£o (sem RNNs ou CNNs)  
* Treinamento altamente paralelizÃ¡vel  
* Mecanismo de auto-atenÃ§Ã£o (self-attention)  
* Produto escalar escalonado (scaled dot-product) para estabilidade  
* AtenÃ§Ã£o multi-cabeÃ§a (multi-head) para mÃºltiplas perspectivas  
* CodificaÃ§Ã£o posicional baseada em seno e cosseno  
* Resultados superiores em traduÃ§Ã£o automÃ¡tica (BLEU)  
* Capacidade de generalizaÃ§Ã£o para outras tarefas de linguagem natural
""")
    if st.button("Jogar Novamente ğŸ”"):
        for key in list(st.session_state.keys()):
            del st.session_state[key]
        st.rerun()
    report_bug_section()

# --- Menu Inicial ---
def main_menu():
    st.title("ğŸš€ A Jornada do Transformer: AtenÃ§Ã£o Desvendada! ğŸš€")
    st.write("Bem-vindo, engenheiro de inteligÃªncia artificial! Sua missÃ£o Ã© guiar um modelo Transformer por cinco fases de construÃ§Ã£o e entendimento. Cada fase abordarÃ¡ um conceito essencial do artigo 'Attention Is All You Need'.")
    try:
        st.image("img/transformer.png", width=200)
    except:
        st.warning("Imagem nÃ£o encontrada.")
    if st.button("Iniciar MissÃ£o â¡ï¸"):
        st.session_state.game_state = "phase1"
        st.rerun()
    report_bug_section()

# --- NavegaÃ§Ã£o ---
st.write(f"ğŸ§­ Estado atual: {st.session_state.game_state}")
if st.session_state.game_state == "menu":
    main_menu()
elif st.session_state.game_state == "phase1":
    phase1_architecture()
elif st.session_state.game_state == "phase2":
    phase2_scaled_dot_product_attention()
elif st.session_state.game_state == "phase3":
    phase3_multi_head_attention()
elif st.session_state.game_state == "phase4":
    phase4_positional_encoding()
elif st.session_state.game_state == "phase5":
    phase5_training_results()
elif st.session_state.game_state == "summary":
    game_summary()
