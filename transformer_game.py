import streamlit as st
import time
import datetime
import os
import numpy as np
import matplotlib.pyplot as plt
import requests

# --- InicializaÃ§Ã£o de Estado ---
def init_state():
    st.session_state.setdefault("game_state", "menu")
    for key in [
        "phase1_passed", "phase2_passed", "phase3_passed", "phase4_passed",
        "show_phase1_feedback", "show_phase2_feedback", "show_phase3_feedback", "show_phase4_feedback"
    ]:
        st.session_state.setdefault(key, False)
    for key in ["p1_attempts", "p2_attempts", "p3_attempts", "p4_attempts"]:
        st.session_state.setdefault(key, 0)

init_state()

# --- Logs de Feedback ---
LOG_FILE = "game_feedback.log"

def log_feedback(feedback_text):
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    try:
        with open(LOG_FILE, "a") as f:
            f.write(f"[{timestamp}] {feedback_text}\n")
        st.success("Obrigado pelo seu feedback! Ele foi registrado.")
    except Exception as e:
        st.error(f"Erro ao salvar feedback: {e}")

# --- FunÃ§Ã£o lateral de bug/sugestÃ£o ---
def report_bug_section():
    st.sidebar.subheader("ğŸ Reportar Erro / SugestÃ£o")
    with st.sidebar.form("bug_report_form", clear_on_submit=True):
        bug_text = st.text_area("Descreva o erro que encontrou ou sua sugestÃ£o de melhoria:")
        submitted = st.form_submit_button("Enviar Feedback âœ‰ï¸")
        if submitted:
            if bug_text.strip():
                log_feedback(bug_text)
            else:
                st.sidebar.warning("Por favor, escreva algo antes de enviar.")

# --- Fase 1: Mini-game de Montagem do Transformer ---
def phase1_architecture():
    st.header("Fase 1: A Arquitetura Fundacional (Encoder-Decoder) ğŸ—ï¸")
    st.write("Arraste os blocos abaixo para a ordem correta da arquitetura Transformer: de entrada atÃ© a saÃ­da.")

    componentes = [
        "Mecanismo de AtenÃ§Ã£o",
        "Camada de SaÃ­da",
        "Decoder",
        "Encoder",
        "Embedding"
    ]  # Ordem embaralhada para evitar sugestÃ£o direta

    ordem_correta = [
        "Embedding", "Encoder", "Mecanismo de AtenÃ§Ã£o", "Decoder", "Camada de SaÃ­da"
    ]

    dicas = [
        "PosiÃ§Ã£o 1 - Transforma cada palavra em um vetor numÃ©rico compreensÃ­vel pela IA.",
        "PosiÃ§Ã£o 2 - Processa a frase de entrada e gera uma representaÃ§Ã£o contextualizada.",
        "PosiÃ§Ã£o 3 - Decide quais palavras sÃ£o mais importantes umas para as outras.",
        "PosiÃ§Ã£o 4 - Gera a frase de saÃ­da, com base na atenÃ§Ã£o e no encoder.",
        "PosiÃ§Ã£o 5 - Traduz a saÃ­da do decoder para palavras compreensÃ­veis."
    ]

    escolhas = []
    for i in range(len(ordem_correta)):
        st.markdown(dicas[i])
        escolha = st.selectbox(f"Escolha para a posiÃ§Ã£o {i + 1}", ["â¬‡ï¸ Escolha"] + componentes, key=f"fase1_{i}")
        escolhas.append(escolha)

    if st.button("Verificar Ordem"):
        if escolhas == ordem_correta:
            st.session_state.phase1_passed = True
            st.session_state.show_phase1_feedback = True
            st.rerun()
        else:
            st.error("âŒ Ainda nÃ£o estÃ¡ certo! Tente organizar os blocos na sequÃªncia lÃ³gica.")

    if st.session_state.get("show_phase1_feedback", False):
        st.success("âœ… Correto! Essa Ã© a ordem de processamento do Transformer.")
        st.image("img/transformer.png", width=300, caption="Arquitetura do Transformer: Encoder-Decoder com AtenÃ§Ã£o")
        if st.button("AvanÃ§ar para Fase 2 â¡ï¸", key="p1_advance_button"):
            st.session_state.game_state = "phase2"
            st.session_state.show_phase1_feedback = False
            st.rerun()

    report_bug_section()

# --- Fase 2 ---
def phase2_scaled_dot_product_attention():
    st.header("Fase 2: Corrida de Vetores e Escalonamento ğŸ¯")
    st.write("Vamos entender como a atenÃ§Ã£o funciona usando vetores. Aqui, vocÃª controla os vetores **Q (Query)**, **K (Key)** e o parÃ¢metro **dâ‚–** (dimensÃ£o da chave).")

    with st.expander("ğŸ¤” O que sÃ£o Q, K e dâ‚–?"):
        st.markdown("""
- **Q (Query - Consulta)**: representa o vetor da palavra que estÃ¡ pedindo informaÃ§Ã£o. Ele pergunta: â€œquais palavras sÃ£o relevantes para mim?â€
- **K (Key - Chave)**: representa cada uma das outras palavras que podem ser relevantes.
- **dâ‚– (dimensÃ£o da chave)**: controla o tamanho dos vetores Q e K. Serve para normalizar o cÃ¡lculo de similaridade.

O cÃ¡lculo da atenÃ§Ã£o Ã© feito assim:
```Attention(Q, K, V) = softmax(QÂ·Káµ— / âˆšdâ‚–)Â·V```
Se QÂ·K for muito grande, a softmax se satura e os gradientes viram quase zero. A divisÃ£o por âˆšdâ‚– evita isso.
        """)

    q_val = st.slider("Valor do vetor Q (intensidade da consulta)", 1, 100, 60, step=1)
    k_val = st.slider("Valor do vetor K (intensidade da chave)", 1, 100, 80, step=1)
    d_k = st.slider("DimensÃ£o dâ‚– (escalonador, tamanho dos vetores)", 1, 100, 64, step=1)

    produto = q_val * k_val
    com_escalonamento = produto / (d_k ** 0.5)

    st.markdown(f"**Produto Escalar (QÂ·K):** `{produto}`")
    st.markdown(f"**Escalonado (Ã· âˆšdâ‚–):** `{com_escalonamento:.2f}`")

    if com_escalonamento < 30:
        st.success("âœ… Perfeito! O escalonamento protege a funÃ§Ã£o softmax de saturar, garantindo gradientes estÃ¡veis.")
        if st.button("AvanÃ§ar para Fase 3 â¡ï¸", key="p2_advance_button"):
            st.session_state.game_state = "phase3"
            st.rerun()
    else:
        st.warning("âš ï¸ O valor escalonado ainda estÃ¡ alto. Isso pode saturar a softmax e impedir o modelo de aprender corretamente. Reduza Q, K ou aumente dâ‚–.")

    report_bug_section()

# --- Fase 3 ---
def phase3_multi_head_attention():
    st.header("Fase 3: Cobrinha Multi-CabeÃ§a ğŸ")
    st.write("Neste mini-jogo, vocÃª vai entender como diferentes 'cabeÃ§as' de atenÃ§Ã£o podem focar em diferentes partes da entrada.")
    st.write("Cada cabeÃ§a deve capturar um tipo de informaÃ§Ã£o em uma frase simplificada.")

    palavras = ["JoÃ£o", "correu", "atÃ©", "a", "loja"]
    opcoes = ["sintaxe", "semÃ¢ntica", "posiÃ§Ã£o"]

    colunas = st.columns(len(palavras))
    atribuicoes = []
    for i in range(len(palavras)):
        with colunas[i]:
            escolha = st.selectbox(f"'{palavras[i]}'", ["--"] + opcoes, key=f"fase3_{i}")
            atribuicoes.append(escolha)

    if st.button("Verificar CabeÃ§as"):
        tipos_usados = set(atribuicoes)
        if all(e in tipos_usados for e in opcoes):
            st.success("âœ… Excelente! Cada cabeÃ§a estÃ¡ capturando uma dimensÃ£o diferente da frase.")
            if st.button("AvanÃ§ar para Fase 4 â¡ï¸", key="p3_advance_button"):
                st.session_state.game_state = "phase4"
                st.rerun()
        else:
            st.warning("âš ï¸ Tente distribuir as atenÃ§Ãµes entre sintaxe, semÃ¢ntica e posiÃ§Ã£o para as palavras.")

    report_bug_section()

# --- Fase 4 ---
def phase4_positional_encoding():
    st.header("Fase 4: Corrida com CodificaÃ§Ã£o Posicional ğŸ“")
    st.write("Aqui, seu token precisa seguir um caminho com saltos senoidais para escapar dos obstÃ¡culos.")
    st.write("O objetivo Ã© simular o comportamento da codificaÃ§Ã£o posicional seno/cosseno usada no Transformer.")

    num_pontos = st.slider("Tamanho da sequÃªncia (tokens)", 5, 50, 20)
    uso_seno = st.radio("Tipo de codificaÃ§Ã£o posicional:", ["Constante", "Linear", "Senoidal"], index=2)

    x = np.arange(num_pontos)
    if uso_seno == "Constante":
        y = np.ones_like(x)
    elif uso_seno == "Linear":
        y = x
    else:
        y = np.sin(x / 5)

    fig, ax = plt.subplots()
    ax.plot(x, y, marker='o')
    ax.set_title("Caminho da posiÃ§Ã£o na sequÃªncia")
    st.pyplot(fig)

    if uso_seno == "Senoidal":
        st.success("âœ… Correto! FunÃ§Ãµes seno e cosseno codificam posiÃ§Ãµes relativas no Transformer.")
        if st.button("AvanÃ§ar para Fase 5 â¡ï¸", key="p4_advance_button"):
            st.session_state.game_state = "phase5"
            st.rerun()
    else:
        st.warning("âš ï¸ A codificaÃ§Ã£o correta Ã© senoidal. Tente novamente.")

    report_bug_section()

# --- Fase 5 ---
def phase5_training_results():
    st.header("Fase 5: Gerencie seu Treinamento âš™ï¸")
    st.write("Nesta fase final, vocÃª Ã© responsÃ¡vel por treinar seu Transformer com os melhores parÃ¢metros.")
    st.write("FaÃ§a escolhas que equilibrem desempenho (BLEU score) e custo computacional.")

    modelo = st.selectbox("Tamanho do modelo", ["Pequeno", "Base", "Grande"], index=1)
    num_cabecas = st.slider("NÃºmero de cabeÃ§as de atenÃ§Ã£o", 2, 16, 8)
    batch_size = st.slider("Tamanho do batch (lote)", 8, 128, 32, step=8)

    if modelo == "Pequeno":
        base_bleu = 25.0
        custo = 1.0
    elif modelo == "Base":
        base_bleu = 27.3
        custo = 2.5
    else:
        base_bleu = 28.4
        custo = 5.0

    ajuste = (num_cabecas / 8) * (batch_size / 32)
    bleu = base_bleu + np.log2(ajuste + 1)
    custo_total = custo * ajuste

    st.markdown(f"**BLEU estimado:** `{bleu:.2f}`")
    st.markdown(f"**Custo estimado (FLOPs):** `{custo_total:.2f}` unidades")

    if bleu >= 28.0:
        st.success("âœ… ParabÃ©ns! VocÃª otimizou bem seu Transformer.")
        if st.button("Ver Resumo das Descobertas ğŸ†"):
            st.session_state.game_state = "summary"
            st.rerun()
    else:
        st.warning("âš ï¸ Seu BLEU score ainda pode melhorar. Tente ajustar os parÃ¢metros!")

    report_bug_section()

# --- Resumo Final + LLM ---
def game_summary():
    st.header("Resumo: Attention Is All You Need ğŸ‰")
    st.write("VocÃª concluiu todas as fases e compreendeu os principais pilares da arquitetura Transformer. Veja abaixo um resumo aprofundado, totalmente alinhado ao artigo original de Vaswani et al. (2017):")

    st.markdown("""
**1. Arquitetura baseada exclusivamente em atenÃ§Ã£o**  
O Transformer elimina redes recorrentes (RNNs) e convolucionais (CNNs), usando atenÃ§Ã£o como base para capturar dependÃªncias entre tokens, permitindo paralelismo.

**2. Treinamento altamente paralelizÃ¡vel**  
Sem processar um token por vez, o modelo acelera o treinamento usando processamento simultÃ¢neo em GPUs/TPUs.

**3. Mecanismo de auto-atenÃ§Ã£o (self-attention)**  
Cada palavra pondera sua relaÃ§Ã£o com todas as outras da sequÃªncia, produzindo uma representaÃ§Ã£o contextual rica.

**4. Produto escalar escalonado**  
Para evitar saturaÃ§Ã£o da softmax e gradientes instÃ¡veis, o produto escalar Ã© dividido por âˆšdâ‚–.

**5. AtenÃ§Ã£o multi-cabeÃ§a**  
MÃºltiplas cabeÃ§as processam diferentes subespaÃ§os de atenÃ§Ã£o em paralelo (sintaxe, semÃ¢ntica etc.).

**6. CodificaÃ§Ã£o posicional senoidal**  
Como nÃ£o hÃ¡ ordem natural, a posiÃ§Ã£o Ã© codificada usando seno e cosseno de frequÃªncias diferentes.

**7. Resultados em BLEU**  
No WMT 2014, o Transformer superou todos os modelos anteriores com BLEU score 27.3 (base) e 28.4 (big).

**8. GeneralizaÃ§Ã£o para outras tarefas**  
Sua estrutura inspirou modelos como BERT, GPT, T5 â€” aplicados em muitas tarefas de NLP.
""")

    st.markdown("---")
    st.subheader("â“ Pergunte sobre Transformers")
    st.write("Use o assistente abaixo para tirar dÃºvidas sobre o conteÃºdo do jogo!")

    pergunta = st.text_area("Digite sua pergunta para o modelo Falcon-7B-Instruct:", key="qa_final")
    if st.button("Responder", key="qa_submit"):
        if pergunta.strip():
            with st.spinner("Gerando resposta..."):
                resposta = requests.post(
                    "https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct",
                    json={"inputs": f"Pergunta: {pergunta}\\nResposta:"}
                )
                if resposta.status_code == 200:
                    saida = resposta.json()[0].get("generated_text", "")
                    st.markdown(saida)
                else:
                    st.error("NÃ£o foi possÃ­vel gerar uma resposta agora.")
        else:
            st.warning("Digite sua pergunta antes de enviar.")

    if st.button("Jogar Novamente ğŸ”"):
        for key in list(st.session_state.keys()):
            del st.session_state[key]
        st.rerun()

    report_bug_section()

# --- Menu Inicial ---
def main_menu():
    st.title("ğŸš€ A Jornada do Transformer: AtenÃ§Ã£o Desvendada! ğŸš€")
    st.write("Bem-vindo, engenheiro de inteligÃªncia artificial! Sua missÃ£o Ã© guiar um modelo Transformer por cinco fases...")
    try:
        st.image("img/transformer.png", width=200)
    except:
        st.warning("Imagem nÃ£o encontrada.")
    if st.button("Iniciar MissÃ£o â¡ï¸"):
        st.session_state.game_state = "phase1"
        st.rerun()
    report_bug_section()

# --- NavegaÃ§Ã£o ---
st.write(f"ğŸ§­ Estado atual: {st.session_state.game_state}")
if st.session_state.game_state == "menu":
    main_menu()
elif st.session_state.game_state == "phase1":
    phase1_architecture()
elif st.session_state.game_state == "phase2":
    phase2_scaled_dot_product_attention()
elif st.session_state.game_state == "phase3":
    phase3_multi_head_attention()
elif st.session_state.game_state == "phase4":
    phase4_positional_encoding()
elif st.session_state.game_state == "phase5":
    phase5_training_results()
elif st.session_state.game_state == "summary":
    game_summary()
