import streamlit as st
import time
import datetime
import os

# --- InicializaÃ§Ã£o de Estado ---
def init_state():
    st.session_state.setdefault("game_state", "menu")
    for key in ["p1_attempts", "p2_attempts", "p3_attempts", "p4_attempts"]:
        st.session_state.setdefault(key, 0)

init_state()

# --- Logs de Feedback ---
LOG_FILE = "game_feedback.log"

def log_feedback(feedback_text):
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    try:
        with open(LOG_FILE, "a") as f:
            f.write(f"[{timestamp}] {feedback_text}\n")
        st.success("Obrigado pelo seu feedback! Ele foi registrado.")
    except Exception as e:
        st.error(f"Erro ao salvar feedback: {e}")

# --- FunÃ§Ã£o lateral de bug/sugestÃ£o ---
def report_bug_section():
    st.sidebar.subheader("\U0001F41E Reportar Erro / SugestÃ£o")
    with st.sidebar.form("bug_report_form", clear_on_submit=True):
        bug_text = st.text_area("Descreva o erro que encontrou ou sua sugestÃ£o de melhoria:")
        submitted = st.form_submit_button("Enviar Feedback âœ‰ï¸", key="feedback_submit_button")
        if submitted:
            if bug_text.strip():
                log_feedback(bug_text)
            else:
                st.sidebar.warning("Por favor, escreva algo antes de enviar.")

# --- Fases do Jogo ---
def main_menu():
    st.title("ğŸš€ A Jornada do Transformer: AtenÃ§Ã£o Desvendada! ğŸš€")
    st.markdown("Bem-vindo, **engenheiro de IA**! Sua missÃ£o Ã© construir o modelo de traduÃ§Ã£o de linguagem mais eficiente e poderoso do mundo. Guie seu **Transformer** atravÃ©s das fases de design, treinamento e otimizaÃ§Ã£o.")
    st.image("https://upload.wikimedia.org/wikipedia/commons/thumb/1/1d/Transformer_architecture.svg/800px-Transformer_architecture.svg.png", use_container_width=True, caption="Arquitetura do Transformer: Onde a AtenÃ§Ã£o Ã© Tudo!")
    st.write("Prepare-se para desvendar os segredos da atenÃ§Ã£o!")
    if st.button("Iniciar MissÃ£o â¡ï¸", key="main_menu_start_button"):
        st.session_state.game_state = "phase1"
        st.rerun()
    report_bug_section()

def phase1_architecture():
    st.header("Fase 1: A Arquitetura Fundacional (Encoder-Decoder) ğŸ—ï¸")
    st.write("Antes do Transformer, a maioria dos modelos de sequÃªncia usava redes recorrentes (RNNs) ou convolucionais (CNNs) para processar informaÃ§Ãµes...")
    col1, col2, col3 = st.columns(3)
    with col1:
        if st.button("Redes Recorrentes Complexas ğŸ”„", key="p1_btn_rnn"):
            st.error("âŒ Resposta incorreta!...")
            st.session_state.p1_attempts += 1
    with col2:
        if st.button("Redes Convolucionais ğŸ–¼ï¸", key="p1_btn_cnn"):
            st.warning("âš ï¸ Resposta aceitÃ¡vel, mas nÃ£o a ideal!...")
            st.session_state.p1_attempts += 1
    with col3:
        if st.button("AtenÃ§Ã£o Pura (Transformer) âœ¨", key="p1_btn_attention"):
            st.success("âœ… Correto!...")
            if st.button("AvanÃ§ar para Fase 2 â¡ï¸", key="p1_advance_button"):
                st.session_state.game_state = "phase2"
                st.rerun()
    if st.session_state.get('p1_attempts', 0) >= 3 and st.session_state.game_state != "phase2":
        st.info("ğŸ’¡ Dica: Lembre-se que o Transformer 'dispensa' recorrÃªncia...")
    report_bug_section()

def phase2_scaled_dot_product_attention():
    st.header("Fase 2: O Poder da AtenÃ§Ã£o (Scaled Dot-Product Attention) ğŸ¯")
    st.write("A funÃ§Ã£o de atenÃ§Ã£o mapeia uma Query (Q)...")
    col1, col2 = st.columns(2)
    with col1:
        if st.button("Aumentar a magnitude dos produtos escalares", key="p2_btn_increase"):
            st.error("âŒ Incorreto!...")
            st.session_state.p2_attempts += 1
    with col2:
        if st.button("Evitar gradientes muito pequenos no softmax âœ…", key="p2_btn_softmax"):
            st.success("âœ… Correto!...")
            if st.button("AvanÃ§ar para Fase 3 â¡ï¸", key="p2_advance_button"):
                st.session_state.game_state = "phase3"
                st.rerun()
    if st.session_state.get('p2_attempts', 0) >= 2 and st.session_state.game_state != "phase3":
        st.info("ğŸ’¡ Dica: Pense no que acontece com os valores quando $d_k$ Ã© grande...")
    report_bug_section()

def phase3_multi_head_attention():
    st.header("Fase 3: Multi-Head Attention: MÃºltiplas Perspectivas ğŸ’¡")
    st.write("Em vez de uma Ãºnica funÃ§Ã£o de atenÃ§Ã£o...")
    d_val = st.slider("Escolha o valor para d_k e d_v (esperado d_model/h)", 32, 128, 64, step=32, key="p3_slider")
    if d_val == 64:
        st.success("âœ… Correto!...")
        if st.button("AvanÃ§ar para Fase 4 â¡ï¸", key="p3_advance_button"):
            st.session_state.game_state = "phase4"
            st.rerun()
    else:
        st.warning("Tente novamente...")
        st.session_state.p3_attempts += 1
        if st.session_state.p3_attempts >= 2:
            st.info("ğŸ’¡ Dica: Divida a dimensÃ£o total (d_model) pela quantidade de cabeÃ§as (h).")
    report_bug_section()

def phase4_positional_encoding():
    st.header("Fase 4: A ImportÃ¢ncia da PosiÃ§Ã£o (Positional Encoding) ğŸ“")
    st.write("Como o Transformer nÃ£o possui recorrÃªncia ou convoluÃ§Ã£o...")
    col1, col2, col3 = st.columns(3)
    with col1:
        if st.button("Embeddings Posicionais Aprendidos", key="p4_btn_learned"):
            st.warning("âš ï¸ O paper experimentou isso...")
            st.session_state.p4_attempts += 1
    with col2:
        if st.button("FunÃ§Ãµes Seno e Cosseno de Diferentes FrequÃªncias âœ…", key="p4_btn_sin_cos"):
            st.success("âœ… Correto!...")
            if st.button("AvanÃ§ar para Fase 5 â¡ï¸", key="p4_advance_button"):
                st.session_state.game_state = "phase5"
                st.rerun()
    with col3:
        if st.button("Hash de PosiÃ§Ã£o", key="p4_btn_hash"):
            st.error("âŒ Incorreto...")
            st.session_state.p4_attempts += 1
    if st.session_state.p4_attempts >= 2 and st.session_state.game_state != "phase5":
        st.info("ğŸ’¡ Dica: O mÃ©todo escolhido foi para permitir extrapolaÃ§Ã£o...")
    report_bug_section()

def phase5_training_results():
    st.header("Fase 5: Treinamento e OtimizaÃ§Ã£o âš¡")
    st.write("Vamos simular o treinamento do seu Transformer...")
    progress_bar = st.progress(0)
    for i in range(100):
        time.sleep(0.01)
        progress_bar.progress(i + 1)
    st.success("âœ… Treinamento ConcluÃ­do!")
    if st.button("Ver Resumo das Descobertas ğŸ†", key="p5_summary_button"):
        st.session_state.game_state = "summary"
        st.rerun()
    report_bug_section()

def game_summary():
    st.header("MissÃ£o ConcluÃ­da! ParabÃ©ns! ğŸ‰")
    st.markdown("* Arquitetura baseada em atenÃ§Ã£o\n* ParalelizaÃ§Ã£o aumentada\n* Auto-atenÃ§Ã£o\n* Multi-Head Attention\n* CodificaÃ§Ã£o Posicional\n* Resultados superiores\n* GeneralizaÃ§Ã£o...")
    if st.button("Jogar Novamente ğŸ”", key="summary_replay_button"):
        for key in st.session_state.keys():
            del st.session_state[key]
        st.rerun()
    report_bug_section()

# --- NavegaÃ§Ã£o entre fases ---
st.write(f"\U0001F9ED Estado atual: {st.session_state.game_state}")

if st.session_state.game_state == "menu":
    main_menu()
elif st.session_state.game_state == "phase1":
    phase1_architecture()
elif st.session_state.game_state == "phase2":
    phase2_scaled_dot_product_attention()
elif st.session_state.game_state == "phase3":
    phase3_multi_head_attention()
elif st.session_state.game_state == "phase4":
    phase4_positional_encoding()
elif st.session_state.game_state == "phase5":
    phase5_training_results()
elif st.session_state.game_state == "summary":
    game_summary()
