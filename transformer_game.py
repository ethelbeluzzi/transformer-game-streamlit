import streamlit as st
import time
import datetime
import os
import numpy as np
import matplotlib.pyplot as plt
import requests

# --- InicializaÃ§Ã£o de Estado ---
def init_state():
    st.session_state.setdefault("game_state", "menu")
    for key in [
        "phase1_passed", "phase2_passed", "phase3_passed", "phase4_passed",
        "show_phase1_feedback", "show_phase2_feedback", "show_phase3_feedback", "show_phase4_feedback"
    ]:
        st.session_state.setdefault(key, False)
    for key in ["p1_attempts", "p2_attempts", "p3_attempts", "p4_attempts"]:
        st.session_state.setdefault(key, 0)

init_state()

# --- Logs de Feedback ---
from github import Github
import datetime
import streamlit as st

def log_feedback(feedback_text):
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    full_text = f"[{timestamp}] {feedback_text}\n"

    token = st.secrets["GITHUB_TOKEN"]
    repo_name = st.secrets["REPO_NAME"]
    file_path = st.secrets["FILE_PATH"]

    g = Github(token)
    repo = g.get_repo(repo_name)

    try:
        contents = repo.get_contents(file_path)
        new_content = contents.decoded_content.decode() + full_text
        repo.update_file(file_path, "append feedback", new_content, contents.sha)
    except Exception:
        repo.create_file(file_path, "create feedback log", full_text)

    st.success("âœ… Feedback salvo com sucesso no repositÃ³rio privado!")

# --- FunÃ§Ã£o lateral de bug/sugestÃ£o ---
def report_bug_section():
    st.sidebar.subheader("ğŸ Reportar Erro Conceitual do Jogo")
    with st.sidebar.form("bug_report_form", clear_on_submit=True):
        bug_text = st.text_area("Descreva o erro que encontrou ou sua sugestÃ£o de melhoria:")
        submitted = st.form_submit_button("Enviar Feedback âœ‰ï¸")
        if submitted:
            if bug_text.strip():
                log_feedback(bug_text)
            else:
                st.sidebar.warning("Por favor, escreva algo antes de enviar.")

# --- FunÃ§Ã£o lateral de llms ---

from huggingface_hub import InferenceClient

import requests

import requests

def llm_sidebar_consultation():
    st.sidebar.subheader("ğŸ¤– Tem alguma dÃºvida? Pergunte aqui para a LLM! (Qwen2.5-7B-Instruct, via Hugging Face)")
    user_question = st.sidebar.text_area("Digite sua dÃºvida abaixo:", key="hf_chat_user_question")

    if st.sidebar.button("Enviar pergunta", key="hf_chat_submit") and user_question.strip():
        with st.spinner("Consultando a LLM..."):
            try:
                hf_token = st.secrets["HF_TOKEN"]
                headers = {
                    "Authorization": f"Bearer {hf_token}",
                    "Content-Type": "application/json"
                }

                API_URL = "https://router.huggingface.co/together/v1/chat/completions"
                payload = {
                    "model": "Qwen/Qwen2.5-7B-Instruct-Turbo",
                    "messages": [
                        {"role": "user", "content": user_question.strip()}
                    ],
                    "temperature": 0.7,
                    "max_tokens": 300
                }

                response = requests.post(API_URL, headers=headers, json=payload, timeout=30)

                if response.status_code == 200:
                    result = response.json()
                    reply = result["choices"][0]["message"]["content"]
                    st.sidebar.success("ğŸ“˜ Resposta da LLM:")
                    st.sidebar.markdown(f"> {reply.strip()}")
                elif response.status_code == 429:
                    st.sidebar.error("âš ï¸ Ops, atingimos o limite de requests para o modelo!")
                else:
                    st.sidebar.error("âŒ Ocorreu um erro inesperado ao consultar a LLM.")

            except Exception as e:
                st.sidebar.error("âŒ Ocorreu um erro tÃ©cnico ao tentar se conectar Ã  LLM.")

    # ğŸ”½ Adiciona separador entre a LLM e a caixa de feedback de erro conceitual
    st.sidebar.markdown("---")


# --- Fase 1: Mini-game de Montagem do Transformer ---
def phase1_architecture():
    st.header("Fase 1: A Arquitetura Fundacional (Encoder-Decoder) ğŸ—ï¸")

    st.markdown("""
> ğŸ“˜ **Conceito-chave do artigo**  
> "Nosso modelo segue a arquitetura geral do transformador como uma pilha de camadas de codificador e decodificador."  
> â€” *Vaswani et al., 2017*

A arquitetura Encoder-Decoder permite que o modelo processe a entrada por completo antes de gerar a saÃ­da, otimizando tarefas como traduÃ§Ã£o, resumo e question answering.
    """)

    st.write("Arraste os blocos abaixo para a ordem correta da arquitetura Transformer: da entrada atÃ© a saÃ­da.")

    componentes = [
        "Mecanismo de AtenÃ§Ã£o",
        "Camada de SaÃ­da",
        "Decoder",
        "Encoder",
        "Embedding"
    ]

    ordem_correta = [
        "Embedding", "Encoder", "Mecanismo de AtenÃ§Ã£o", "Decoder", "Camada de SaÃ­da"
    ]

    dicas = [
        "PosiÃ§Ã£o 1 - **Embedding**: transforma cada palavra em um vetor numÃ©rico compreensÃ­vel pela IA.",
        "PosiÃ§Ã£o 2 - **Encoder**: processa a frase de entrada e gera uma representaÃ§Ã£o contextualizada.",
        "PosiÃ§Ã£o 3 - **Mecanismo de AtenÃ§Ã£o**: decide quais palavras sÃ£o mais importantes umas para as outras.",
        "PosiÃ§Ã£o 4 - **Decoder**: gera a frase de saÃ­da, com base na atenÃ§Ã£o e no encoder.",
        "PosiÃ§Ã£o 5 - **Camada de SaÃ­da**: traduz a saÃ­da do decoder para palavras compreensÃ­veis."
    ]

    escolhas = []
    for i in range(len(ordem_correta)):
        st.markdown(dicas[i])
        escolha = st.selectbox(f"Escolha para a posiÃ§Ã£o {i + 1}", ["â¬‡ï¸ Escolha"] + componentes, key=f"fase1_{i}")
        escolhas.append(escolha)

    if st.button("Verificar Ordem"):
        if escolhas == ordem_correta:
            st.session_state.phase1_passed = True
            st.session_state.show_phase1_feedback = True
            st.rerun()
        else:
            st.error("âŒ Ainda nÃ£o estÃ¡ certo! Tente organizar os blocos na sequÃªncia lÃ³gica.")

    if st.session_state.get("show_phase1_feedback", False):
        st.success("âœ… Correto! Essa Ã© a ordem de processamento do Transformer.")
        st.image("img/transformer.png", width=300, caption="Arquitetura do Transformer: Encoder-Decoder com AtenÃ§Ã£o")
        if st.button("AvanÃ§ar para Fase 2 â¡ï¸", key="p1_advance_button"):
            st.session_state.game_state = "phase2"
            st.session_state.show_phase1_feedback = False
            st.rerun()

    st.markdown("""
> ğŸ”¬ **AlÃ©m do artigo**  
> Modelos como **T5**, **BART** e muitos sistemas modernos de traduÃ§Ã£o neural usam variantes dessa arquitetura.  
> A separaÃ§Ã£o clara entre codificaÃ§Ã£o e decodificaÃ§Ã£o facilita o **aprendizado transferido (transfer learning)**, a modularizaÃ§Ã£o e a adaptaÃ§Ã£o para tarefas distintas â€” como sumarizaÃ§Ã£o, diÃ¡logo e atÃ© geraÃ§Ã£o de cÃ³digo.
    """)

    llm_sidebar_consultation()
    report_bug_section()


# --- Fase 2 ---
def phase2_scaled_dot_product_attention():
    st.header("Fase 2: Corrida de Vetores e Escalonamento ğŸ¯")

    st.markdown("""
> ğŸ“˜ **Conceito-chave do artigo**  
> "Utilizamos atenÃ§Ã£o por produto escalar escalonado, que Ã© rÃ¡pida e eficiente em termos de espaÃ§o computacional."  
> â€” *Vaswani et al., 2017*

A divisÃ£o por âˆšdâ‚– evita que os valores da softmax se tornem extremos, preservando gradientes Ãºteis para aprendizado. Essa operaÃ§Ã£o Ã© fundamental para a estabilidade da rede durante o treinamento.
    """)

    with st.expander("ğŸ¤” O que sÃ£o Q, K e dâ‚–?"):
        st.markdown("""
- **Q (Query - Consulta):** Representa o vetor da palavra que estÃ¡ buscando contexto.  
- **K (Key - Chave):** Representa as palavras candidatas a fornecer esse contexto.  
- **dâ‚– (dimensÃ£o da chave):** Tamanho dos vetores Q e K.  
- Se dâ‚– for grande, os produtos QÂ·K podem saturar a softmax. Por isso escalonamos.
        """)
        st.markdown("A fÃ³rmula da atenÃ§Ã£o Ã©:")
        st.latex(r"Attention(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V")

    q_val = st.slider("Valor do vetor Q (intensidade da consulta)", 1, 100, 60, step=1)
    k_val = st.slider("Valor do vetor K (intensidade da chave)", 1, 100, 80, step=1)
    d_k = st.slider("DimensÃ£o dâ‚– (tamanho do vetor)", 1, 128, 64, step=1)

    produto = q_val * k_val
    com_escalonamento = produto / (d_k ** 0.5)

    st.markdown(f"**Produto Escalar (QÂ·K):** `{produto}`")
    st.markdown(f"**Com Escalonamento (Ã· âˆšdâ‚–):** `{com_escalonamento:.2f}`")

    if 10 <= com_escalonamento <= 30:
        st.success("âœ… Excelente! O valor escalonado estÃ¡ em uma faixa ideal para o funcionamento do softmax.")
        st.info("ğŸ“˜ Dica: valores entre **10 e 30** mantÃªm a softmax balanceada e os gradientes Ãºteis.")
        if st.button("AvanÃ§ar para Fase 3 â¡ï¸", key="p2_advance_button"):
            st.session_state.game_state = "phase3"
            st.rerun()
    else:
        st.warning("âš ï¸ O valor escalonado ainda estÃ¡ fora do ideal. Tente ajustar Q, K ou dâ‚– para obter resultado entre **10 e 30**.")

    st.markdown("""
> ğŸ”¬ **AlÃ©m do artigo**  
> A dimensÃ£o dos vetores **Q e K** afeta a expressividade da atenÃ§Ã£o:  
> - Vetores **pequenos** (ex: 16, 32) nÃ£o capturam nuances complexas.  
> - Vetores **grandes demais** (ex: 128, 256) causam produtos exagerados â†’ saturaÃ§Ã£o da softmax â†’ aprendizado prejudicado.  
>  
> A escalagem por âˆšdâ‚– **compensa esse efeito**, mantendo os gradientes estÃ¡veis.  
>  
> Na prÃ¡tica, isso Ã© essencial em **modelos como GPT ou T5**, que processam sequÃªncias longas e dependem de uma atenÃ§Ã£o estÃ¡vel para manter coerÃªncia sem degradar o aprendizado em passos distantes.
    """)

    llm_sidebar_consultation()
    report_bug_section()

# --- Fase 3 ---
def phase3_multi_head_attention():
    st.header("Fase 3: Multi-Head Attention: CabeÃ§as Paralelas ğŸ§ ")

    st.markdown("""
> ğŸ“˜ **Conceito-chave do artigo**  
> "Ao invÃ©s de uma Ãºnica atenÃ§Ã£o com vetores de dimensÃ£o dâ‚˜â‚’dâ‚‘â‚—, projetamos Q, K, V mÃºltiplas vezes (h cabeÃ§as) para subespaÃ§os menores, permitindo que o modelo atenda simultaneamente a diferentes informaÃ§Ãµes de diferentes posiÃ§Ãµes."  
> â€” *Vaswani et al., 2017*

A Multi-Head Attention permite que o Transformer olhe para a mesma informaÃ§Ã£o de diversas maneiras simultaneamente, aprendendo padrÃµes variados entre tokens.
    """)

    frase = ["O", "modelo", "aprende", "relaÃ§Ãµes", "entre", "tokens"]
    st.write("Escolha uma palavra para observar como diferentes cabeÃ§as podem reagir a ela:")

    foco = st.selectbox("Palavra de foco (query)", frase, key="p3_query")

    padroes_cabeca1 = {
        "O": ["modelo"],
        "modelo": ["O", "aprende"],
        "aprende": ["modelo"],
        "relaÃ§Ãµes": ["entre"],
        "entre": ["relaÃ§Ãµes"],
        "tokens": ["entre"]
    }

    padroes_cabeca2 = {
        "O": ["aprende"],
        "modelo": ["relaÃ§Ãµes"],
        "aprende": ["tokens"],
        "relaÃ§Ãµes": ["modelo"],
        "entre": ["aprende"],
        "tokens": ["O"]
    }

    padroes_cabeca3 = {
        "O": ["O"],
        "modelo": ["tokens"],
        "aprende": ["relaÃ§Ãµes"],
        "relaÃ§Ãµes": ["tokens"],
        "entre": ["modelo"],
        "tokens": ["relaÃ§Ãµes"]
    }

    st.markdown("ğŸ” **CabeÃ§a 1** (posiÃ§Ã£o local): tende a olhar para palavras vizinhas.")
    st.markdown("ğŸ” **CabeÃ§a 2** (ligaÃ§Ã£o estrutural): conecta palavras com dependÃªncia gramatical.")
    st.markdown("ğŸ” **CabeÃ§a 3** (semÃ¢ntica implÃ­cita): foca em termos semanticamente relacionados.")

    st.markdown("---")
    st.markdown(f"ğŸ§  Com foco em **{foco}**, veja como cada cabeÃ§a pode responder:")

    st.write(f"**CabeÃ§a 1:** AtenÃ§Ã£o distribuÃ­da para: {', '.join(padroes_cabeca1.get(foco, []))}")
    st.write(f"**CabeÃ§a 2:** AtenÃ§Ã£o distribuÃ­da para: {', '.join(padroes_cabeca2.get(foco, []))}")
    st.write(f"**CabeÃ§a 3:** AtenÃ§Ã£o distribuÃ­da para: {', '.join(padroes_cabeca3.get(foco, []))}")

    st.success("âœ… Observe como diferentes cabeÃ§as focam em padrÃµes distintos â€” essa diversidade Ã© essencial para que o modelo compreenda mÃºltiplas relaÃ§Ãµes contextuais ao mesmo tempo.")

    if st.button("AvanÃ§ar para Fase 4 â¡ï¸", key="p3_advance_button"):
        st.session_state.game_state = "phase4"
        st.rerun()

    st.markdown("""
> ğŸ”¬ **AlÃ©m do artigo**  
> Em modelos maiores como **GPT-3 ou PaLM**, o nÃºmero de cabeÃ§as cresce (ex: 96 ou mais).  
> Cada uma aprende de forma independente:  
> - Algumas especializam-se em pontuaÃ§Ã£o, outras em coesÃ£o, ou em longas dependÃªncias sintÃ¡ticas.  
> - A diversidade entre cabeÃ§as Ã© essencial para tarefas como sumarizaÃ§Ã£o, programaÃ§Ã£o, traduÃ§Ã£o ou raciocÃ­nio matemÃ¡tico.  
>  
> Mesmo cabeÃ§as com desempenho fraco isoladamente podem ser Ãºteis dentro do conjunto.
    """)

    llm_sidebar_consultation()
    report_bug_section()

# --- Fase 4 ---
import numpy as np
import matplotlib.pyplot as plt

def phase4_positional_encoding():
    st.header("Fase 4: CodificaÃ§Ã£o Posicional (Positional Encoding) ğŸŒ")

    st.markdown("""
> ğŸ“˜ **Conceito-chave do artigo**  
> â€œComo o modelo nÃ£o possui mecanismos recorrentes ou convolucionais, Ã© necessÃ¡rio incorporar alguma informaÃ§Ã£o sobre a ordem das palavras na sequÃªncia. Para isso, usamos funÃ§Ãµes senoidais que variam com a posiÃ§Ã£o.â€  
> â€” *Vaswani et al., 2017*

Transformers nÃ£o tÃªm noÃ§Ã£o da ordem dos tokens por padrÃ£o. Para isso, adicionam aos embeddings vetores de **codificaÃ§Ã£o posicional** â€” combinaÃ§Ãµes de seno e cosseno â€” que representam a posiÃ§Ã£o de cada palavra na sequÃªncia.

Essas funÃ§Ãµes produzem padrÃµes contÃ­nuos e diferenciÃ¡veis, permitindo que o modelo:
- ReconheÃ§a a **posiÃ§Ã£o absoluta** dos tokens
- Codifique **relaÃ§Ãµes de distÃ¢ncia** entre palavras
- **Extrapole** para comprimentos de sequÃªncia maiores que os vistos no treino
""")

    st.subheader("ğŸ”¢ VisualizaÃ§Ã£o: Senoides para representar posiÃ§Ãµes")

    posicoes = np.arange(0, 50)
    dim = 16  # Exemplo com 16 dimensÃµes
    pos_enc = np.array([
        [np.sin(p / (10000 ** (2 * i / dim))) if i % 2 == 0 else np.cos(p / (10000 ** (2 * (i - 1) / dim))) for i in range(dim)]
        for p in posicoes
    ])

    fig, ax = plt.subplots(figsize=(8, 3))
    ax.plot(posicoes, pos_enc[:, 0], label="DimensÃ£o 0 (seno)")
    ax.plot(posicoes, pos_enc[:, 1], label="DimensÃ£o 1 (cosseno)")
    ax.set_title("VariaÃ§Ã£o senoidal em duas dimensÃµes do Positional Encoding")
    ax.set_xlabel("PosiÃ§Ã£o do token")
    ax.legend()
    st.pyplot(fig)

    st.markdown("Acima, vemos como diferentes dimensÃµes oscilam de forma distinta conforme a posiÃ§Ã£o muda. Isso cria um **padrÃ£o Ãºnico** por posiÃ§Ã£o, que pode ser aprendido pelo modelo.")

    st.subheader("ğŸ§  Pergunta")
    resposta = st.radio("O que o Positional Encoding permite ao Transformer?", [
        "Capturar a importÃ¢ncia semÃ¢ntica das palavras",
        "Aprender a ordem e a distÃ¢ncia entre os tokens",
        "Entender a frequÃªncia de cada palavra",
        "Ignorar a posiÃ§Ã£o, jÃ¡ que a atenÃ§Ã£o cuida disso"
    ], index=0, key="fase4_radio")

    if resposta:
        if resposta == "Aprender a ordem e a distÃ¢ncia entre os tokens":
            st.success("âœ… Correto! O Positional Encoding insere padrÃµes que permitem ao modelo saber quem vem antes ou depois, e quÃ£o longe cada palavra estÃ¡ da outra.")
            if st.button("AvanÃ§ar para Fase 5 â¡ï¸", key="p4_advance_button"):
                st.session_state.game_state = "phase5"
                st.rerun()
        else:
            st.error("âŒ Ainda nÃ£o! Lembre-se: o objetivo do Positional Encoding Ã© oferecer ao modelo uma forma de representar a **ordem e distÃ¢ncia** entre tokens â€” algo que, sozinho, a atenÃ§Ã£o nÃ£o captura.")

    st.markdown("""
> ğŸ”¬ **AlÃ©m do artigo**  
> Muitos modelos modernos (como BERT e GPT) usam variantes de codificaÃ§Ã£o posicional:  
> - **Fixas** (como seno/cosseno) â†’ extrapolam para posiÃ§Ãµes alÃ©m do treino  
> - **Aprendidas** â†’ mais flexÃ­veis, mas menos interpretÃ¡veis  
>  
> A codificaÃ§Ã£o posicional continua sendo uma das maiores inovaÃ§Ãµes dos Transformers â€” e uma das razÃµes para sua escalabilidade.
    """)

    llm_sidebar_consultation()
    report_bug_section()


# --- Fase 5 ---
def phase5_training_results():
    st.header("Fase 5: Treinamento e OtimizaÃ§Ã£o (Resultados e EficiÃªncia) âš¡")

    st.markdown("""
> ğŸ“˜ **Conceito-chave do artigo**  
> "O modelo Transformer atinge resultados de ponta em traduÃ§Ã£o automÃ¡tica, com menor custo computacional de treinamento comparado a modelos anteriores."  
> â€” *Vaswani et al., 2017*

A arquitetura baseada em atenÃ§Ã£o pura permite paralelismo eficiente e melhora a escalabilidade, reduzindo o tempo e custo de treinamento mesmo com grande volume de dados.
    """)

    st.subheader("Simulando Treinamento... â³")
    progress_bar = st.progress(0)
    for i in range(100):
        time.sleep(0.01)
        progress_bar.progress(i + 1)
    st.success("âœ… Treinamento ConcluÃ­do! Seu Transformer estÃ¡ pronto!")

    st.write("Aqui estÃ£o os resultados comparativos do Transformer em tarefas de traduÃ§Ã£o (WMT 2014 EN-DE):")

    st.markdown("""
**Legenda:**
- ğŸŸ¢ BLEU Score: quanto mais alto, melhor
- ğŸ”µ FLOPs (Floating Point Operations): quanto menor, mais eficiente
    """)

    data = {
        "Modelo": ["ByteNet", "GNMT + RL", "ConvS2S", "Transformer (base)", "Transformer (big)"],
        "BLEU (EN-DE)": ["23.75", "24.6", "25.16", "**27.3** ğŸŸ¢", "**28.4** ğŸŸ¢"],
        "Custo de Treinamento (FLOPs)": ["$2.3\\cdot10^{19}$", "$1.4\\cdot10^{21}$", "$9.6\\cdot10^{18}$", "**$3.3\\cdot10^{18}$** ğŸ”µ", "**$2.3\\cdot10^{19}$**"]
    }
    st.table(data)

    # ğŸ” Mini ranking
    if st.button("ğŸ” Destacar o melhor modelo"):
        st.info("ğŸ† **Transformer (big)** se destaca com **BLEU 28.4** e excelente desempenho em traduÃ§Ã£o automÃ¡tica!")

    st.markdown("""
> ğŸ”¬ **AlÃ©m do artigo**  
> O BLEU Score Ã© uma mÃ©trica baseada em n-gramas que compara a saÃ­da gerada com traduÃ§Ãµes humanas.  
> - Um aumento de **2 BLEU** pode representar uma diferenÃ§a **perceptÃ­vel na fluÃªncia e precisÃ£o**.  
> - O Transformer nÃ£o sÃ³ superou modelos anteriores, mas o fez com muito **menos custo de FLOPs**.  

Isso abriu caminho para aplicaÃ§Ãµes em tempo real, como traduÃ§Ã£o simultÃ¢nea, assistentes virtuais multilÃ­ngues e atÃ© geraÃ§Ã£o de cÃ³digo (com adaptaÃ§Ãµes).
    """)

    st.success("ğŸš€ Sua missÃ£o foi cumprida com sucesso: vocÃª treinou um Transformer de ponta!")

    if st.button("Ver Resumo Final ğŸ†", key="p5_summary_button"):
        st.session_state.game_state = "summary"
        st.rerun()

    llm_sidebar_consultation()
    report_bug_section()


# --- Resumo Final + LLM ---
def game_summary():
    st.header("MissÃ£o ConcluÃ­da! RecapitulaÃ§Ã£o do artigo 'Attention Is All You Need' ğŸ‰")
    st.subheader("ğŸ§  VocÃª demonstrou uma compreensÃ£o sÃ³lida dos fundamentos do Transformer!")

    st.markdown("""
> ğŸ“˜ **Conceito central do artigo**  
> "A arquitetura Transformer depende exclusivamente de mecanismos de atenÃ§Ã£o, eliminando o uso de recorrÃªncia e convoluÃ§Ã£o, permitindo paralelizaÃ§Ã£o eficiente."  
> â€” *Vaswani et al., 2017*
    """)

    st.markdown("### ğŸ§© Elementos centrais explorados no jogo")

    st.markdown("""
#### 1. **Arquitetura Encoder-Decoder baseada em atenÃ§Ã£o**
- O modelo Ã© organizado em **camadas empilhadas** de codificadores e decodificadores.
- O **Encoder** transforma a entrada em uma representaÃ§Ã£o contextual.
- O **Decoder** gera a saÃ­da com base nessa representaÃ§Ã£o e nas posiÃ§Ãµes anteriores.
- Isso permite lidar com **tarefas de traduÃ§Ã£o**, sumarizaÃ§Ã£o e outras sequenciais com alta flexibilidade.
""")

    st.markdown("""
#### 2. **Mecanismo de AtenÃ§Ã£o por Produto Escalar Escalonado**
- A atenÃ§Ã£o compara a *query* com todas as *keys* e pondera os *values*.
- O produto QÂ·K Ã© **escalonado por âˆšdâ‚–**, evitando saturaÃ§Ã£o da funÃ§Ã£o softmax.
- Isso mantÃ©m os **gradientes Ãºteis** e o **treinamento estÃ¡vel**, mesmo em modelos grandes.
""")

    st.markdown("""
#### 3. **AtenÃ§Ã£o Multi-CabeÃ§a (Multi-Head Attention)**
- Em vez de uma Ãºnica atenÃ§Ã£o, o modelo usa mÃºltiplas cabeÃ§as independentes.
- Cada cabeÃ§a aprende um padrÃ£o diferente: **estrutura, semÃ¢ntica, posiÃ§Ã£o, dependÃªncias**.
- No final, os resultados sÃ£o **concatenados** e projetados novamente, enriquecendo a representaÃ§Ã£o.
""")

    st.markdown("""
#### 4. **Positional Encoding**
- Como o Transformer **nÃ£o possui recorrÃªncia**, ele precisa saber a posiÃ§Ã£o das palavras.
- Usando **funÃ§Ãµes seno e cosseno**, cada posiÃ§Ã£o recebe uma curva Ãºnica, contÃ­nua e extrapolÃ¡vel.
- Isso permite ao modelo lidar com **ordem das palavras** mesmo em contextos longos ou fora da distribuiÃ§Ã£o.
""")

    st.markdown("""
#### 5. **EficiÃªncia de Treinamento e Resultados**
- O Transformer atinge **BLEU scores superiores** a modelos anteriores com **menos FLOPs**.
- A ausÃªncia de recorrÃªncia permite **paralelizaÃ§Ã£o total** no treinamento.
- Sua eficiÃªncia abriu caminho para modelos massivos como BERT, GPT, T5, e muitos outros.
""")

    st.markdown("### ğŸŒ Impactos no mundo real")
    st.markdown("""
- Permitiu o surgimento de modelos de linguagem de cÃ³digo aberto e escalÃ¡veis.
- Influenciou modelos em **Ã¡udio, visÃ£o computacional, bioinformÃ¡tica e robÃ³tica**.
- Tornou possÃ­vel o treinamento em **paralelo em GPUs e TPUs**, reduzindo drasticamente o tempo de inferÃªncia.

> ğŸ”¬ O Transformer mudou profundamente o paradigma de modelagem de linguagem â€” e sua missÃ£o hoje mostra que vocÃª compreende as engrenagens por trÃ¡s dessa revoluÃ§Ã£o.
    """)

    if st.button("Jogar novamente ğŸ”", key="summary_replay_button"):
        for key in list(st.session_state.keys()):
            del st.session_state[key]
        st.rerun()

    llm_sidebar_consultation()
    report_bug_section()

# --- Menu Inicial ---
def main_menu():
    st.title("ğŸš€ A Jornada do Transformer: AtenÃ§Ã£o Desvendada! ğŸš€")

    st.write("""
Esse Ã© um jogo interativo pensado para ajudar vocÃª a revisar, de forma leve e engajada, os principais conceitos do paper clÃ¡ssico *Attention is All You Need*. [Leia o paper original](https://arxiv.org/abs/1706.03762).

Durante o jogo, vocÃª serÃ¡ guiado por cinco fases, cada uma com um mini-desafio sobre aspectos fundamentais do Transformer: arquitetura, atenÃ§Ã£o escalonada, atenÃ§Ã£o multi-cabeÃ§a, codificaÃ§Ã£o posicional e resultados de desempenho.

No canto lateral esquerdo, vocÃª pode:
- â“ Consultar uma **LLM integrada** sempre que tiver dÃºvidas sobre os conceitos apresentados.
- ğŸ Usar a **caixinha de feedback** para reportar erros conceituais ou sugerir melhorias a qualquer momento.
    """)

    # ğŸ”½ Centralizar imagem com layout de colunas
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.image("img/transformer.png", use_container_width=True)

    if st.button("Iniciar MissÃ£o â¡ï¸"):
        st.session_state.game_state = "phase1"
        st.rerun()
    report_bug_section()

# --- NavegaÃ§Ã£o ---
fases_nomes = {
    "menu": "Menu Inicial",
    "phase1": "Fase 1",
    "phase2": "Fase 2",
    "phase3": "Fase 3",
    "phase4": "Fase 4",
    "phase5": "Fase 5",
    "summary": "Resumo Final"
}
estado_legivel = fases_nomes.get(st.session_state.game_state, "Desconhecido")
st.write(f"ğŸ§­ Estado atual: {estado_legivel}")

if st.session_state.game_state == "menu":
    main_menu()
elif st.session_state.game_state == "phase1":
    phase1_architecture()
elif st.session_state.game_state == "phase2":
    phase2_scaled_dot_product_attention()
elif st.session_state.game_state == "phase3":
    phase3_multi_head_attention()
elif st.session_state.game_state == "phase4":
    phase4_positional_encoding()
elif st.session_state.game_state == "phase5":
    phase5_training_results()
elif st.session_state.game_state == "summary":
    game_summary()
